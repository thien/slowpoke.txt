
@article{schaeffer_solving_1996,
	title = {Solving the {Game} of {Checkers}},
	volume = {29},
	url = {http://books.google.com/books?hl=en&lr=&id=cYB-ra2T8i4C&oi=fnd&pg=PA119&dq=Solving+the+Game+of+Checkers&ots=H0lLXM9_gp&sig=0DO68kXG6UsGhvhLfW1a5vqz3n0},
	abstract = {In 1962, a checkers-playing program written by Arthur Samuel defeated a self-proclaimed master player, creating a sensation at the time for the fledgling field of computer science called artificial intelligence. The historical record refers to this event as having solved the game of checkers. This paper discusses achieving three different levels of solving the game: publicly (as evidenced by Samuel's results), practically (by the checkers program Chinook , the best player in the world) and provably (by consid-ering the 5 × 10 20 positions in the search space). The latter definition may be attainable in the near future.},
	journal = {Games of No Chance},
	author = {Schaeffer, Jonathan and Lake, Robert},
	year = {1996},
	pages = {119--133},
	file = {Attachment:/home/tn/Zotero/storage/4JP6WH7G/Schaeffer, Lake - 1996 - Solving the Game of Checkers.pdf:application/pdf}
}

@article{silver_mastering_2017,
	title = {Mastering the game of {Go} without human knowledge},
	volume = {550},
	issn = {0028-0836},
	url = {https://www.nature.com/nature/journal/v550/n7676/full/nature24270.html},
	doi = {10.1038/nature24270},
	abstract = {A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo's own move selections and also the winner of AlphaGo's games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100–0 against the previously published, champion-defeating AlphaGo.},
	journal = {Nature},
	author = {Silver, David and Schrittweiser, Julian and Simonyan, Karen},
	year = {2017},
	pmid = {29052630},
	pages = {354--359},
	file = {Attachment:/home/tn/Zotero/storage/LGJPF2YF/Silver, Schrittweiser, Simonyan - 2017 - Mastering the game of Go without human knowledge.pdf:application/pdf}
}

@phdthesis{al-khateeb_importance_nodate,
	title = {The {Importance} of a {Piece} {Difference} {Feature} to {Blondie}24},
	author = {Al-khateeb, Belal and Kendall, Graham},
	file = {Attachment:/home/tn/Zotero/storage/525QZ87G/Al-khateeb, Kendall - Unknown - The Importance of a Piece Difference Feature to Blondie24.pdf:application/pdf}
}

@article{al-khateeb_introducing_2012,
	title = {Introducing {Individual} and {Social} learning into {Evolutionary} {Checkers}},
	volume = {4},
	number = {4},
	journal = {IEEE Transactions on Computational Intelligence and AI In Games},
	author = {Al-khateeb, Belal and Kendall, Graham},
	year = {2012},
	pages = {258--269},
	file = {Attachment:/home/tn/Zotero/storage/IUXIWJGK/Al-khateeb, Kendall - 2012 - Introducing Individual and Social learning into Evolutionary Checkers.pdf:application/pdf}
}

@article{al-khateeb_introducing_2009,
	title = {Introducing a round robin tournament into blondie24},
	volume = {44},
	doi = {10.1109/CIG.2009.5286487},
	number = {0},
	journal = {CIG2009 - 2009 IEEE Symposium on Computational Intelligence and Games},
	author = {Al-Khateeb, Belal and Kendall, Graham},
	year = {2009},
	pages = {112--116},
	file = {Attachment:/home/tn/Zotero/storage/8JWQG9T8/Al-Khateeb, Kendall - 2009 - Introducing a round robin tournament into blondie24.pdf:application/pdf}
}

@article{stanley_evolving_2002,
	title = {Evolving {Neural} {Networks} through {Augmenting} {Topologies}},
	volume = {10},
	issn = {1063-6560},
	url = {http://www.mitpressjournals.org/doi/10.1162/106365602320169811},
	doi = {10.1162/106365602320169811},
	abstract = {The ventral visual pathway implements object recognition and categorization in a hierarchy of processing areas with neuronal selectivities of increasing complexity. The presence of massive feedback connections within this hierarchy raises the possibility that normal visual processing relies on the use of computational loops. It is not known, however, whether object recognition can be performed at all without such loops (i.e., in a purely feed-forward mode). By analyzing the time course of reaction times in a masked natural scene categorization paradigm, we show that the human visual system can generate selective motor responses based on a single feed-forward pass. We confirm these results using a more constrained letter discrimination task, in which the rapid succession of a target and mask is actually perceived as a distractor. We show that a masked stimulus presented for only 26 msecand often not consciously perceivedcan fully determine the earliest selective motor responses: The neural representations of the stimulus and mask are thus kept separated during a short period corresponding to the feedforward sweep. Therefore, feedback loops do not appear to be mandatory for visual processing. Rather, we found that such loops allow the masked stimulus to reverberate in the visual system and affect behavior for nearly 150 msec after the feed-forward sweep.},
	number = {2},
	journal = {Evolutionary Computation},
	author = {Stanley, Kenneth O. and Miikkulainen, Risto},
	year = {2002},
	pmid = {12180173},
	pages = {99--127},
	file = {Attachment:/home/tn/Zotero/storage/3ZUJA7MJ/Stanley, Miikkulainen - 2002 - Evolving Neural Networks through Augmenting Topologies.pdf:application/pdf}
}

@phdthesis{al-khateeb_investigating_2011,
	title = {Investigating evolutionary checkers by incorporating individual and social learning , {N}-tuple systems and a round robin tournament},
	abstract = {the game of checkers by introducing a league structure into the learning phase of a system based on Blondie24. We believe that this helps eliminate some of the randomness in the evolution. The best player obtained is tested against an evolutionary checkers program based on Blondie24. The results obtained are promising. In addition, we introduce an individual and social learning mechanism into the learning phase of the evolutionary checkers system. The best player obtained is tested against an implementation of an evolutionary checkers program, and also against a player, which utilises a round robin tournament. The results are promising.},
	school = {University of Nottingham},
	author = {Al-Khateeb, Belal},
	year = {2011},
	file = {Attachment:/home/tn/Zotero/storage/CXXBAXYP/Al-Khateeb - 2011 - Investigating evolutionary checkers by incorporating individual and social learning , N-tuple systems and a round ro.pdf:application/pdf}
}

@article{nguyen_multiple_2004,
	title = {Multiple neural networks for a long term time series forecast},
	volume = {13},
	issn = {09410643},
	doi = {10.1007/s00521-003-0390-z},
	abstract = {The artificial neural network (ANN) methodology has been used in various\${\textbackslash}backslash\$ntime series prediction applications. However, the accuracy of a\${\textbackslash}backslash\$nneural network model may be seriously compromised when it is used\${\textbackslash}backslash\$nrecursively for making long-term multi-step predictions. This study\${\textbackslash}backslash\$npresents a method using multiple ANNs to make a long term time series\${\textbackslash}backslash\$nprediction. A multiple neural network (MNN) model is a group of\${\textbackslash}backslash\$nneural networks that work together to solve a problem. In the proposed\${\textbackslash}backslash\$nMNN approach, each component neural network makes forecasts at a\${\textbackslash}backslash\$ndifferent length of time ahead. The MNN method was applied to the\${\textbackslash}backslash\$nproblem of forecasting an hourly customer demand for gas at a compression\${\textbackslash}backslash\$nstation in Saskatchewan, Canada. The results showed that a MNN model\${\textbackslash}backslash\$nperformed better than a single ANN model for long term prediction.},
	number = {1},
	journal = {Neural Computing and Applications},
	author = {Nguyen, Hanh H. and Chan, Christine W.},
	year = {2004},
	keywords = {Multiple neural networks, Time series forecasting},
	pages = {90--98},
	file = {Attachment:/home/tn/Zotero/storage/MUHR9USV/Nguyen, Chan - 2004 - Multiple neural networks for a long term time series forecast.pdf:application/pdf}
}

@article{al-khateeb_effect_2012,
	title = {Effect of look-ahead depth in evolutionary checkers},
	volume = {27},
	issn = {10009000},
	doi = {10.1007/s11390-012-1280-6},
	abstract = {Intuitively it would seem to be the case that any learning algorithm would perform better if it was allowed to search deeper in the game tree. However, there has been some discussion as to whether the evaluation function or the depth of the search is the main contributory factor in the performance of the player. There has been some evidence suggesting that look-ahead (i.e. depth of search) is particularly important. In this work we provide a rigorous set of experiments, which support this view. We believe this is the first time such an intensive study has been carried out for evolutionary checkers. Our experiments show that increasing the depth of a look-ahead has significant improvements to the performance of the checkers program and has a significant effect on its learning abilities.},
	number = {5},
	journal = {Journal of Computer Science and Technology},
	author = {Al-Khateeb, Belal and Kendall, Graham},
	year = {2012},
	keywords = {Evolutionary checkers, Look-ahead depth, Neural network},
	pages = {996--1006},
	file = {Attachment:/home/tn/Zotero/storage/4BEV5XVA/Al-Khateeb, Kendall - 2012 - Effect of look-ahead depth in evolutionary checkers.pdf:application/pdf}
}

@article{samuel_studies_1959,
	title = {Some studies in machine learning using the game of checkers},
	volume = {3},
	issn = {0018-8646},
	url = {https://link.springer.com/content/pdf/10.1007%2F978-1-4613-8716-9_14.pdf},
	doi = {10.1147/rd.441.0206},
	abstract = {Two machine-learning procedures have been investigated in some detail using the game of checkers. Enough work has been done by verify the fact that a computer can be programmed so that it will learn to play a better game of checkers than can be played by the person who wrote the program. Furthermore, it can learn to do this in a remarkably short period of time (8 or 10 hours of machine-playing time) when given only the rules of the game, a sense of direction, and a redundant and incomplete list of parameters which are thought to have something to do with the game, but whose correct signs and relative weights are unknown and unspecified. The principles of machine learning verified by these experiments are, of course, applicable to many other situations.},
	journal = {IBM Journal of Research and Development},
	author = {Samuel, A L},
	year = {1959},
	pages = {210--229},
	file = {Attachment:/home/tn/Zotero/storage/56UN45RQ/Samuel - 2000 - Some studies in machine learning using the game of checkers.pdf:application/pdf}
}

@techreport{cobbe_accelerating_nodate,
	title = {Accelerating {Checkers} {AI} {Evolution}},
	url = {http://cs229.stanford.edu/proj2011/CobbeLeeGomez-AcceleratingCheckersAIEvolution.pdf},
	institution = {Stanford University},
	author = {Cobbe, Karl and Lee, Paul and Gomez-Emilsson, A},
	pages = {2--5},
	file = {Attachment:/home/tn/Zotero/storage/ZKFMTDQP/Cobbe, Lee, Gomez-Emilsson - Unknown - Accelerating Checkers AI Evolution.pdf:application/pdf}
}

@article{winands_monte_2010,
	title = {Monte {Carlo} {Tree} {Search} in {Lines} of {Action}},
	volume = {2},
	issn = {1943-068X},
	doi = {10.1109/TCIAIG.2010.2061050},
	abstract = {The success of Monte Carlo tree search (MCTS) in many games, where αβ-based search has failed, naturally raises the question whether Monte Carlo simulations will eventually also outperform traditional game-tree search in game domains where αβ -based search is now successful. The forte of αβ-based search are highly tactical deterministic game domains with a small to moderate branching factor, where efficient yet knowledge-rich evaluation functions can be applied effectively. In this paper, we describe an MCTS-based program for playing the game Lines of Action (LOA), which is a highly tactical slow-progression game exhibiting many of the properties difficult for MCTS. The program uses an improved MCTS variant that allows it to both prove the game-theoretical value of nodes in a search tree and to focus its simulations better using domain knowledge. This results in simulations superior in both handling tactics and ensuring game progression. Using the improved MCTS variant, our program is able to outperform even the world's strongest αβ-based LOA program. This is an important milestone for MCTS because the traditional game-tree search approach has been considered to be the better suited for playing LOA.},
	number = {4},
	journal = {IEEE Transactions on Computational Intelligence and AI in Games},
	author = {Winands, M. H. M. and Bjornsson, Y. and Saito, J. T.},
	month = dec,
	year = {2010},
	keywords = {computer games, Filling, game theory, game-theoretical value, game-tree search, Game-tree solver, Imaging phantoms, lines of action, Lines of Action (LOA), Monte Carlo methods, Monte Carlo simulations, Monte Carlo tree search, Monte Carlo tree search (MCTS), tactical slow-progression game, tree searching, αβ-based search},
	pages = {239--250},
	file = {IEEE Xplore Abstract Record:/home/tn/Zotero/storage/XDJICKYR/5523941.html:text/html}
}

@article{such_deep_2017,
	title = {Deep {Neuroevolution}: {Genetic} {Algorithms} {Are} a {Competitive} {Alternative} for {Training} {Deep} {Neural} {Networks} for {Reinforcement} {Learning}},
	shorttitle = {Deep {Neuroevolution}},
	url = {http://arxiv.org/abs/1712.06567},
	abstract = {Deep artificial neural networks (DNNs) are typically trained via gradient-based learning algorithms, namely backpropagation. Evolution strategies (ES) can rival backprop-based algorithms such as Q-learning and policy gradients on challenging deep reinforcement learning (RL) problems. However, ES can be considered a gradient-based algorithm because it performs stochastic gradient descent via an operation similar to a finite-difference approximation of the gradient. That raises the question of whether non-gradient-based evolutionary algorithms can work at DNN scales. Here we demonstrate they can: we evolve the weights of a DNN with a simple, gradient-free, population-based genetic algorithm (GA) and it performs well on hard deep RL problems, including Atari and humanoid locomotion. The Deep GA successfully evolves networks with over four million free parameters, the largest neural networks ever evolved with a traditional evolutionary algorithm. These results (1) expand our sense of the scale at which GAs can operate, (2) suggest intriguingly that in some cases following the gradient is not the best choice for optimizing performance, and (3) make immediately available the multitude of techniques that have been developed in the neuroevolution community to improve performance on RL problems. To demonstrate the latter, we show that combining DNNs with novelty search, which was designed to encourage exploration on tasks with deceptive or sparse reward functions, can solve a high-dimensional problem on which reward-maximizing algorithms (e.g. DQN, A3C, ES, and the GA) fail. Additionally, the Deep GA parallelizes better than ES, A3C, and DQN, and enables a state-of-the-art compact encoding technique that can represent million-parameter DNNs in thousands of bytes.},
	urldate = {2018-01-22},
	journal = {arXiv:1712.06567 [cs]},
	author = {Such, Felipe Petroski and Madhavan, Vashisht and Conti, Edoardo and Lehman, Joel and Stanley, Kenneth O. and Clune, Jeff},
	month = dec,
	year = {2017},
	note = {arXiv: 1712.06567},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv\:1712.06567 PDF:/home/tn/Zotero/storage/WN4W2TV8/Such et al. - 2017 - Deep Neuroevolution Genetic Algorithms Are a Comp.pdf:application/pdf;arXiv.org Snapshot:/home/tn/Zotero/storage/Q4FIR48N/1712.html:text/html}
}

@article{conti_improving_2017,
	title = {Improving {Exploration} in {Evolution} {Strategies} for {Deep} {Reinforcement} {Learning} via a {Population} of {Novelty}-{Seeking} {Agents}},
	url = {http://arxiv.org/abs/1712.06560},
	abstract = {Evolution strategies (ES) are a family of black-box optimization algorithms able to train deep neural networks roughly as well as Q-learning and policy gradient methods on challenging deep reinforcement learning (RL) problems, but are much faster (e.g. hours vs. days) because they parallelize better. However, many RL problems require directed exploration because they have reward functions that are sparse or deceptive (i.e. contain local optima), and it is not known how to encourage such exploration with ES. Here we show that algorithms that have been invented to promote directed exploration in small-scale evolved neural networks via populations of exploring agents, specifically novelty search (NS) and quality diversity (QD) algorithms, can be hybridized with ES to improve its performance on sparse or deceptive deep RL tasks, while retaining scalability. Our experiments confirm that the resultant new algorithms, NS-ES and a version of QD we call NSR-ES, avoid local optima encountered by ES to achieve higher performance on tasks ranging from playing Atari to simulated robots learning to walk around a deceptive trap. This paper thus introduces a family of fast, scalable algorithms for reinforcement learning that are capable of directed exploration. It also adds this new family of exploration algorithms to the RL toolbox and raises the interesting possibility that analogous algorithms with multiple simultaneous paths of exploration might also combine well with existing RL algorithms outside ES.},
	urldate = {2018-01-22},
	journal = {arXiv:1712.06560 [cs]},
	author = {Conti, Edoardo and Madhavan, Vashisht and Such, Felipe Petroski and Lehman, Joel and Stanley, Kenneth O. and Clune, Jeff},
	month = dec,
	year = {2017},
	note = {arXiv: 1712.06560},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv\:1712.06560 PDF:/home/tn/Zotero/storage/M7RWGFCM/Conti et al. - 2017 - Improving Exploration in Evolution Strategies for .pdf:application/pdf;arXiv.org Snapshot:/home/tn/Zotero/storage/XPTZVEAH/1712.html:text/html}
}

@article{lehman_safe_2017,
	title = {Safe {Mutations} for {Deep} and {Recurrent} {Neural} {Networks} through {Output} {Gradients}},
	url = {http://arxiv.org/abs/1712.06563},
	abstract = {While neuroevolution (evolving neural networks) has a successful track record across a variety of domains from reinforcement learning to artificial life, it is rarely applied to large, deep neural networks. A central reason is that while random mutation generally works in low dimensions, a random perturbation of thousands or millions of weights is likely to break existing functionality, providing no learning signal even if some individual weight changes were beneficial. This paper proposes a solution by introducing a family of safe mutation (SM) operators that aim within the mutation operator itself to find a degree of change that does not alter network behavior too much, but still facilitates exploration. Importantly, these SM operators do not require any additional interactions with the environment. The most effective SM variant capitalizes on the intriguing opportunity to scale the degree of mutation of each individual weight according to the sensitivity of the network's outputs to that weight, which requires computing the gradient of outputs with respect to the weights (instead of the gradient of error, as in conventional deep learning). This safe mutation through gradients (SM-G) operator dramatically increases the ability of a simple genetic algorithm-based neuroevolution method to find solutions in high-dimensional domains that require deep and/or recurrent neural networks (which tend to be particularly brittle to mutation), including domains that require processing raw pixels. By improving our ability to evolve deep neural networks, this new safer approach to mutation expands the scope of domains amenable to neuroevolution.},
	urldate = {2018-01-22},
	journal = {arXiv:1712.06563 [cs]},
	author = {Lehman, Joel and Chen, Jay and Clune, Jeff and Stanley, Kenneth O.},
	month = dec,
	year = {2017},
	note = {arXiv: 1712.06563},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv\:1712.06563 PDF:/home/tn/Zotero/storage/NS8BFCGC/Lehman et al. - 2017 - Safe Mutations for Deep and Recurrent Neural Netwo.pdf:application/pdf;arXiv.org Snapshot:/home/tn/Zotero/storage/ZKFT4RQE/1712.html:text/html}
}

@inproceedings{baier_monte-carlo_2013,
	title = {Monte-{Carlo} tree search and minimax hybrids},
	booktitle = {Computational {Intelligence} in {Games} ({CIG}), 2013 {IEEE} {Conference} on},
	publisher = {IEEE},
	author = {Baier, Hendrik and Winands, Mark HM},
	year = {2013},
	pages = {1--8},
	file = {paper 49.pdf:/home/tn/Zotero/storage/CZWHXZGN/paper 49.pdf:application/pdf}
}

@inproceedings{nair_rectified_2010,
	title = {Rectified linear units improve restricted boltzmann machines},
	booktitle = {Proceedings of the 27th international conference on machine learning ({ICML}-10)},
	author = {Nair, Vinod and Hinton, Geoffrey E.},
	year = {2010},
	pages = {807--814},
	file = {reluICML.pdf:/home/tn/Zotero/storage/KQTHHAUM/reluICML.pdf:application/pdf}
}

@phdthesis{allsop_artficial_2013,
	title = {Artficial {Intelligence} {Techniques} {Applied} {To} {Draughts}},
	school = {Durham University},
	author = {Allsop, Daniel (Durham University)},
	year = {2013},
	file = {Attachment:/home/tn/Zotero/storage/VN5S9LCQ/Allsop - 2013 - Artficial Intelligence Techniques Applied To Draughts Artificial Intelligence Techniques Applied To Draughts.pdf:application/pdf}
}

@inproceedings{montana_training_1989,
	title = {Training {Feedforward} {Neural} {Networks} {Using} {Genetic} {Algorithms}.},
	volume = {89},
	booktitle = {{IJCAI}},
	author = {Montana, David J. and Davis, Lawrence},
	year = {1989},
	pages = {762--767},
	file = {122.pdf:/home/tn/Zotero/storage/EZIWHW97/122.pdf:application/pdf}
}

@article{browne_survey_2012,
	title = {A {Survey} of {Monte} {Carlo} {Tree} {Search} {Methods}},
	volume = {4},
	issn = {1943-068X},
	doi = {10.1109/TCIAIG.2012.2186810},
	abstract = {Monte Carlo tree search (MCTS) is a recently proposed search method that combines the precision of tree search with the generality of random sampling. It has received considerable interest due to its spectacular success in the difficult problem of computer Go, but has also proved beneficial in a range of other domains. This paper is a survey of the literature to date, intended to provide a snapshot of the state of the art after the first five years of MCTS research. We outline the core algorithm's derivation, impart some structure on the many variations and enhancements that have been proposed, and summarize the results from the key game and nongame domains to which MCTS methods have been applied. A number of open research questions indicate that the field is ripe for future work.},
	number = {1},
	journal = {IEEE Transactions on Computational Intelligence and AI in Games},
	author = {Browne, C. B. and Powley, E. and Whitehouse, D. and Lucas, S. M. and Cowling, P. I. and Rohlfshagen, P. and Tavener, S. and Perez, D. and Samothrakis, S. and Colton, S.},
	month = mar,
	year = {2012},
	keywords = {game theory, Monte Carlo methods, Monte Carlo tree search (MCTS), tree searching, Artificial intelligence, Artificial intelligence (AI), bandit-based methods, computer Go, Computers, Decision theory, game search, Game theory, Games, key game, Markov processes, MCTS research, Monte carlo tree search methods, nongame domains, random sampling generality, upper confidence bounds (UCB), upper confidence bounds for trees (UCT)},
	pages = {1--43},
	file = {IEEE Xplore Abstract Record:/home/tn/Zotero/storage/X2QAE6CV/6145622.html:text/html}
}

@article{chellapilla_evolving_1999,
	title = {Evolving {Neural} {Networks} to {Play} {Checkers} without {Expert} {Knowledge}},
	volume = {10},
	issn = {10459227},
	url = {http://ieeexplore.ieee.org/abstract/document/809083},
	doi = {10.1109/72.809083},
	abstract = {— An experiment was conducted where neural net-works compete for survival in an evolving population based on their ability to play checkers. More specifically, multilayer feedforward neural networks were used to evaluate alternative board positions and games were played using a minimax search strategy. At each generation, the extant neural networks were paired in competitions and selection was used to eliminate those that performed poorly relative to other networks. Offspring neural networks were created from the survivors using random variation of all weights and bias terms. After a series of 250 generations, the best-evolved neural network was played against human opponents in a series of 90 games on an internet website. The neural network was able to defeat two expert-level players and played to a draw against a master. The final rating of the neural network placed it in the " Class A " category using a standard rating system. Of particular importance in the design of the experiment was the fact that no features beyond the piece differential were given to the neural networks as a priori knowledge. The process of evolution was able to extract all of the additional information required to play at this level of competency. It accomplished this based almost solely on the feedback offered in the final aggregated outcome of each game played (i.e., win, lose, or draw). This procedure stands in marked contrast to the typical artifice of explicitly injecting expert knowledge into a game-playing program. Index Terms—Alpha–beta search, checkers, evolutionary com-putation, feedforward neural networks, game playing.},
	number = {6},
	journal = {IEEE Transactions on Neural Networks},
	author = {Chellapilla, Kumar and Fogel, David},
	year = {1999},
	pmid = {18252639},
	pages = {1382--1391},
	file = {Attachment:/home/tn/Zotero/storage/UIECZQLX/Chellapilla, Fogel - 1999 - Evolving Neural Networks to Play Checkers without Expert Knowledge.pdf:application/pdf}
}

@article{lorentz_using_2016,
	title = {Using evaluation functions in {Monte}-{Carlo} {Tree} {Search}},
	volume = {644},
	issn = {03043975},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0304397516302717},
	doi = {10.1016/j.tcs.2016.06.026},
	language = {en},
	urldate = {2018-01-06},
	journal = {Theoretical Computer Science},
	author = {Lorentz, Richard},
	month = sep,
	year = {2016},
	pages = {106--113},
	file = {1-s2.0-S0304397516302717-main.pdf:/home/tn/Zotero/storage/LF6KY5WA/1-s2.0-S0304397516302717-main.pdf:application/pdf}
}

@article{tong_genetic_2010,
	title = {Genetic {Algorithm}-{Neural} {Network} ({GANN}): a study of neural network activation functions and depth of genetic algorithm search applied to feature selection},
	volume = {1},
	issn = {1868-8071, 1868-808X},
	shorttitle = {Genetic {Algorithm}-{Neural} {Network} ({GANN})},
	url = {http://link.springer.com/10.1007/s13042-010-0004-x},
	doi = {10.1007/s13042-010-0004-x},
	language = {en},
	number = {1-4},
	urldate = {2018-01-01},
	journal = {International Journal of Machine Learning and Cybernetics},
	author = {Tong, Dong Ling and Mintram, Robert},
	month = dec,
	year = {2010},
	pages = {75--87},
	file = {10.1007s13042-010-0004-x.pdf:/home/tn/Zotero/storage/D28IPPEH/10.1007s13042-010-0004-x.pdf:application/pdf}
}

@misc{otoro_labs_visual_2017,
	type = {Blog},
	title = {A {Visual} {Guide} to {Evolution} {Strategies}},
	url = {http://blog.otoro.net/2017/10/29/visual-evolution-strategies/},
	urldate = {2017-11-12},
	journal = {Otoro},
	author = {Otoro Labs},
	month = oct,
	year = {2017}
}

@article{franken_comparing_2003,
	title = {Comparing {PSO} structures to learn the game of checkers from zero knowledge},
	volume = {1},
	doi = {10.1109/CEC.2003.1299580},
	abstract = {This paper investigates the effectiveness of various particle swarm optimiser structures to learn how to play the game of checkers. Co-evolutionary techniques are used to train the game playing agents. Performance is compared against a player making moves at random. Initial experimental results indicate definite advantages in using certain information sharing structures and swarm size configurations to successfully learn the game of checkers.},
	journal = {2003 Congress on Evolutionary Computation, CEC 2003 - Proceedings},
	author = {Franken, Nelis and Engelbrecht, Andries P.},
	year = {2003},
	pages = {234--241},
	file = {Attachment:/home/tn/Zotero/storage/H3RUZQJH/Franken, Engelbrecht - 2003 - Comparing PSO structures to learn the game of checkers from zero knowledge.pdf:application/pdf}
}

@article{klambauer_self-normalizing_2017,
	title = {Self-{Normalizing} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1706.02515},
	doi = {1706.02515},
	abstract = {Deep Learning has revolutionized vision via convolutional neural networks (CNNs) and natural language processing via recurrent neural networks (RNNs). However, success stories of Deep Learning with standard feed-forward neural networks (FNNs) are rare. FNNs that perform well are typically shallow and, therefore cannot exploit many levels of abstract representations. We introduce self-normalizing neural networks (SNNs) to enable high-level abstract representations. While batch normalization requires explicit normalization, neuron activations of SNNs automatically converge towards zero mean and unit variance. The activation function of SNNs are "scaled exponential linear units" (SELUs), which induce self-normalizing properties. Using the Banach fixed-point theorem, we prove that activations close to zero mean and unit variance that are propagated through many network layers will converge towards zero mean and unit variance – even under the presence of noise and perturbations. This convergence property of SNNs allows to (1) train deep networks with many layers, (2) employ strong regularization, and (3) to make learning highly robust. Furthermore, for activations not close to unit variance, we prove an upper and lower bound on the variance, thus, vanishing and exploding gradients are impossible. We compared SNNs on (a) 121 tasks from the UCI machine learning repository, on (b) drug discovery benchmarks, and on (c) astronomy tasks with standard FNNs and other machine learning methods such as random forests and support vector machines. SNNs significantly outperformed all competing FNN methods at 121 UCI tasks, outperformed all competing methods at the Tox21 dataset, and set a new record at an astronomy data set. The winning SNN architectures are often very deep. Implementations are available at: github.com/bioinf-jku/SNNs.},
	journal = {arXiv},
	author = {Klambauer, Günter and Unterthiner, Thomas and Mayr, Andreas and Hochreiter, Sepp},
	year = {2017},
	file = {Attachment:/home/tn/Zotero/storage/BRBMAXR2/Klambauer et al. - 2017 - Self-Normalizing Neural Networks.pdf:application/pdf}
}

@book{fogel_blondie24:_2001,
	title = {Blondie24: {Playing} at the {Edge} of {AI}},
	isbn = {978-1-55860-783-5},
	url = {http://www.sciencedirect.com/science/article/pii/B9781558607835500167},
	abstract = {This chapter examines the expertise of Blondie24 in playing games. Blondie24 played more than 110 out of 165 on zone.com for six weeks. Overall, she played 84 games as red and 81 as white in which her average rating was 2,045.85 with a standard deviation of 33.94. After 165 games, the trajectory had stabilized, and Blondie was clearly declared an expert. As a result, Blondie's got a place in top 500 out of 12,000 registered players on zone.com. Blondie was better than 99.61 percent of all the rated players at the website. Blondie made a remarkable 84 wins, 20 draws, and 11 losses when played against the opponents who were rated below 2,000. Moreover, Blondie had 10 wins, 12 draws, and 22 losses when played with the opponents rated from 2,000 to 2,200. When Blondie played with other professionals, she won 31.25 percent of her games.},
	author = {Fogel, David B},
	year = {2001},
	doi = {10.1016/B978-155860783-5/50016-7},
	pmid = {12488991},
	file = {Attachment:/home/tn/Zotero/storage/5SFN69E4/Fogel - 2001 - Blondie24 Playing at the Edge of AI.pdf:application/pdf}
}

@book{schaeffer_one_1997,
	title = {One {Jump} {Ahead}},
	isbn = {978-0-387-76575-4},
	author = {Schaeffer, Jonathan},
	year = {1997},
	file = {Attachment:/home/tn/Zotero/storage/Q2CJ735V/Schaeffer - 1997 - One Jump Ahead.pdf:application/pdf}
}

@phdthesis{koehn_combining_1994,
	address = {The University Of Tennessee, Knoxville},
	type = {Thesis},
	title = {Combining {Genetic} {Algorithms} and {Neural} {Networks} : {The} {Encoding} {Problem}},
	abstract = {Neural networks and genetic algorithms demonstrate powerful problem solving ability. They are based on quite simple principles, but take advantage of their mathematical nature: non-linear iteration. Neural networks with backpropagation learning showed results by searching for various kinds of functions. However, the choice of the basic parameter (network topology, learning rate, initial weights) often already determines the success of the training process. The selection of these parame- ter follow in practical use rules of thumb, but their value is at most arguable. Genetic algorithms are global search methods, that are based on princi- ples like selection, crossover and mutation. This thesis examines how genetic algorithms can be used to optimize the network topology etc. of neural net- works. It investigates, how various encoding strategies influence the GA/NN synergy. They are evaluated according to their performance on academic and practical problems of different complexity.},
	school = {The University Of Tennessee, Knoxville},
	author = {Koehn, Philipp},
	year = {1994},
	file = {Attachment:/home/tn/Zotero/storage/NUFQSZFS/Thesis - 1994 - Combining Genetic Algorithms and Neural Networks The Encoding Problem.pdf:application/pdf}
}

@article{perez_apply_nodate,
	title = {Apply genetic algorithm to the learning phase of a neural network},
	abstract = {Natural networks have been used during several years to solve classifica-tion problems. The performance of a neural network depends directly on the design of the hidden layers, and in the calculation of the weights that connect the different nodes. On this project, the structure of the hidden layer is not modified, as the interest lies only on the calculation of the weights of the system. In order to obtain a feasible result, the weights of the neural network are calculated due a function cost. A genetic algo-rithm approach is presented and compared with the gradient descent in failure rate and time to obtain a solution.},
	author = {Perez, Sergi},
	file = {Attachment:/home/tn/Zotero/storage/YLCQSUUZ/Perez - Unknown - Apply genetic algorithm to the learning phase of a neural network.pdf:application/pdf}
}

@article{pappa_contrasting_2014,
	title = {Contrasting meta-learning and hyper-heuristic research: the role of evolutionary algorithms},
	volume = {15},
	issn = {1573-7632},
	url = {https://doi.org/10.1007/s10710-013-9186-9},
	doi = {10.1007/s10710-013-9186-9},
	abstract = {The fields of machine meta-learning and hyper-heuristic optimisation have developed mostly independently of each other, although evolutionary algorithms (particularly genetic programming) have recently played an important role in the development of both fields. Recent work in both fields shares a common goal, that of automating as much of the algorithm design process as possible. In this paper we first provide a historical perspective on automated algorithm design, and then we discuss similarities and differences between meta-learning in the field of supervised machine learning (classification) and hyper-heuristics in the field of optimisation. This discussion focuses on the dimensions of the problem space, the algorithm space and the performance measure, as well as clarifying important issues related to different levels of automation and generality in both fields. We also discuss important research directions, challenges and foundational issues in meta-learning and hyper-heuristic research. It is important to emphasize that this paper is not a survey, as several surveys on the areas of meta-learning and hyper-heuristics (separately) have been previously published. The main contribution of the paper is to contrast meta-learning and hyper-heuristics methods and concepts, in order to promote awareness and cross-fertilisation of ideas across the (by and large, non-overlapping) different communities of meta-learning and hyper-heuristic researchers. We hope that this cross-fertilisation of ideas can inspire interesting new research in both fields and in the new emerging research area which consists of integrating those fields.},
	number = {1},
	journal = {Genetic Programming and Evolvable Machines},
	author = {Pappa, Gisele L. and Ochoa, Gabriela and Hyde, Matthew R. and Freitas, Alex A. and Woodward, John and Swan, Jerry},
	month = mar,
	year = {2014},
	pages = {3--35},
	file = {Attachment:/home/tn/Zotero/storage/F3BICFJC/Pappa et al. - 2014 - Contrasting meta-learning and hyper-heuristic research The role of evolutionary algorithms.pdf:application/pdf}
}

@inproceedings{kusiak_evolutionary_2007,
	address = {Berlin, Heidelberg},
	series = {{ICANNGA} '07},
	title = {Evolutionary {Approach} to the {Game} of {Checkers}},
	isbn = {978-3-540-71589-4},
	url = {http://dx.doi.org/10.1007/978-3-540-71618-1_48},
	doi = {10.1007/978-3-540-71618-1_48},
	abstract = {A new method of genetic evolution of linear and nonlinear evaluation functions in the game of checkers is presented. Several practical issues concerning application of genetic algorithms for this task are pointed out and discussed. Experimental results confirm that proposed approach leads to efficient evaluation functions comparable to the ones used in some of commercial applications.},
	urldate = {2018-03-21},
	booktitle = {Proceedings of the 8th {International} {Conference} on {Adaptive} and {Natural} {Computing} {Algorithms}, {Part} {I}},
	publisher = {Springer-Verlag},
	author = {Kusiak, Magdalena and Walędzik, Karol and Mańdziuk, Jacek},
	year = {2007},
	pages = {432--440},
	file = {Attachment:/home/tn/Zotero/storage/QDLCFTSD/Kusiak, Walezedik, Jacek - 2007 - Evolutionary approach to the game of checkers.pdf:application/pdf}
}

@article{fogel_evolving_2000,
	title = {Evolving a {Checkers} {Player} {Without} {Relying} on {Human} {Experience}},
	volume = {11},
	issn = {1523-8822},
	url = {http://doi.acm.org/10.1145/337897.337996},
	doi = {10.1145/337897.337996},
	number = {2},
	urldate = {2018-03-21},
	journal = {Intelligence},
	author = {Fogel, David B.},
	month = jun,
	year = {2000},
	pages = {20--27},
	file = {ACM Full Text PDF:/home/tn/Zotero/storage/IHI2GJXT/Fogel - 2000 - Evolving a Checkers Player Without Relying on Huma.pdf:application/pdf;Attachment:/home/tn/Zotero/storage/5NYJH8NH/Fogel - 2000 - Evolving a Checkers Player.pdf:application/pdf}
}

@article{montana_neural_1995,
	title = {Neural network weight selection using genetic algorithms},
	volume = {8},
	number = {6},
	journal = {Intelligent Hybrid Systems},
	author = {Montana, David J.},
	year = {1995},
	pages = {12--19},
	file = {hybridslides.pdf:/home/tn/Zotero/storage/XS5R4HCQ/hybridslides.pdf:application/pdf}
}

@phdthesis{elyasaf_evolving_2014,
	type = {Thesis},
	title = {Evolving {Hyper}-{Heuristics} using {Genetic} {Programming}},
	abstract = {The application of computational intelligence techniques within the vast domain  of games has been increasing at a breathtaking speed. Over the past few years  my research has produced a plethora of results in games of different natures, ev-  idencing the success and efficiency of evolutionary algorithms in general—and ge-  netic programming in particular—at producing top-notch, human-competitive game  strategies.  Studying games may advance our knowledge both in cognition and artificial  intelligence, and, last but not least, games possess a competitive angle that coincides  with our human nature, thus motivating researchers.  In this dissertation I explore the application of genetic programming to the devel-  opment of search heuristics for difficult games. I apply GP to the evolution of solvers  for the Rush Hour puzzle and the game of FreeCell, along the way demonstrating a  general method for evolving heuristics.},
	school = {Ben-Gurion University of the Negev},
	author = {Elyasaf, Achiya},
	month = oct,
	year = {2014},
	file = {Attachment:/home/tn/Zotero/storage/MP58ZU8M/Elyasaf - 2014 - Evolving Hyper-Heuristics using Genetic Programming.pdf:application/pdf}
}

@article{xie_genetic_2017,
	title = {Genetic {CNN}},
	url = {http://arxiv.org/abs/1703.01513},
	abstract = {The deep Convolutional Neural Network (CNN) is the state-of-the-art solution for large-scale visual recognition. Following basic principles such as increasing the depth and constructing highway connections, researchers have manually designed a lot of fixed network structures and verified their effectiveness. In this paper, we discuss the possibility of learning deep network structures automatically. Note that the number of possible network structures increases exponentially with the number of layers in the network, which inspires us to adopt the genetic algorithm to efficiently traverse this large search space. We first propose an encoding method to represent each network structure in a fixed-length binary string, and initialize the genetic algorithm by generating a set of randomized individuals. In each generation, we define standard genetic operations, e.g., selection, mutation and crossover, to eliminate weak individuals and then generate more competitive ones. The competitiveness of each individual is defined as its recognition accuracy, which is obtained via training the network from scratch and evaluating it on a validation set. We run the genetic process on two small datasets, i.e., MNIST and CIFAR10, demonstrating its ability to evolve and find high-quality structures which are little studied before. These structures are also transferrable to the large-scale ILSVRC2012 dataset.},
	urldate = {2018-03-21},
	journal = {arXiv:1703.01513 [cs]},
	author = {Xie, Lingxi and Yuille, Alan},
	month = mar,
	year = {2017},
	note = {arXiv: 1703.01513},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Submitted to CVPR 2017 (10 pages, 5 figures)},
	file = {arXiv\:1703.01513 PDF:/home/tn/Zotero/storage/PVDRLE2Y/Xie and Yuille - 2017 - Genetic CNN.pdf:application/pdf;arXiv.org Snapshot:/home/tn/Zotero/storage/BJ4K3V4Y/1703.html:text/html}
}

@article{lai_giraffe:_2015,
	title = {Giraffe: {Using} {Deep} {Reinforcement} {Learning} to {Play} {Chess}},
	shorttitle = {Giraffe},
	url = {http://arxiv.org/abs/1509.01549},
	abstract = {This report presents Giraffe, a chess engine that uses self-play to discover all its domain-specific knowledge, with minimal hand-crafted knowledge given by the programmer. Unlike previous attempts using machine learning only to perform parameter-tuning on hand-crafted evaluation functions, Giraffe's learning system also performs automatic feature extraction and pattern recognition. The trained evaluation function performs comparably to the evaluation functions of state-of-the-art chess engines - all of which containing thousands of lines of carefully hand-crafted pattern recognizers, tuned over many years by both computer chess experts and human chess masters. Giraffe is the most successful attempt thus far at using end-to-end machine learning to play chess.},
	urldate = {2018-03-21},
	journal = {arXiv:1509.01549 [cs]},
	author = {Lai, Matthew},
	month = sep,
	year = {2015},
	note = {arXiv: 1509.01549},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: MSc Dissertation},
	file = {arXiv\:1509.01549 PDF:/home/tn/Zotero/storage/CQA57DWD/Lai - 2015 - Giraffe Using Deep Reinforcement Learning to Play.pdf:application/pdf;arXiv.org Snapshot:/home/tn/Zotero/storage/YW9YISRA/1509.html:text/html}
}

@article{toledo-marin_is_2016,
	title = {Is a good offensive always the best defense?},
	url = {http://arxiv.org/abs/1608.07223},
	abstract = {A checkers-like model game with a simplified set of rules is studied through extensive simulations of agents with different expertise and strategies. The introduction of complementary strategies, in a quite general way, provides a tool to mimic the basic ingredients of a wide scope of real games. We find that only for the player having the higher offensive expertise (the dominant player ), maximizing the offensive always increases the probability to win. For the non-dominant player, interestingly, a complete minimization of the offensive becomes the best way to win in many situations, depending on the relative values of the defense expertise. Further simulations on the interplay of defense expertise were done separately, in the context of a fully-offensive scenario, offering a starting point for analytical treatments. In particular, we established that in this scenario the total number of moves is defined only by the player with the lower defensive expertise. We believe that these results stand for a first step towards a new way to improve decisions-making in a large number of zero-sum real games.},
	urldate = {2018-03-21},
	journal = {arXiv:1608.07223 [cs]},
	author = {Toledo-Marín, J. Quetzalcóatl and Díaz-Méndez, Rogelio and Mussot, Marcelo del Castillo},
	month = aug,
	year = {2016},
	note = {arXiv: 1608.07223},
	keywords = {Computer Science - Artificial Intelligence},
	annote = {Comment: 12 pages, 12 figures},
	file = {arXiv\:1608.07223 PDF:/home/tn/Zotero/storage/Z6A6NTBZ/Toledo-Marín et al. - 2016 - Is a good offensive always the best defense.pdf:application/pdf;arXiv.org Snapshot:/home/tn/Zotero/storage/KL6XE6NY/1608.html:text/html}
}

@article{schaeffer_checkers_2007,
	title = {Checkers {Is} {Solved}},
	volume = {317},
	copyright = {American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	url = {http://science.sciencemag.org.ezphost.dur.ac.uk/content/317/5844/1518},
	doi = {10.1126/science.1144079},
	abstract = {The game of checkers has roughly 500 billion billion possible positions (5 × 1020). The task of solving the game, determining the final result in a game with no mistakes made by either player, is daunting. Since 1989, almost continuously, dozens of computers have been working on solving checkers, applying state-of-the-art artificial intelligence techniques to the proving process. This paper announces that checkers is now solved: Perfect play by both sides leads to a draw. This is the most challenging popular game to be solved to date, roughly one million times as complex as Connect Four. Artificial intelligence technology has been used to generate strong heuristic-based game-playing programs, such as Deep Blue for chess. Solving a game takes this to the next level by replacing the heuristics with perfection.},
	language = {en},
	number = {5844},
	urldate = {2018-03-21},
	journal = {Science},
	author = {Schaeffer, Jonathan and Burch, Neil and Björnsson, Yngvi and Kishimoto, Akihiro and Müller, Martin and Lake, Robert and Lu, Paul and Sutphen, Steve},
	month = sep,
	year = {2007},
	pmid = {17641166},
	pages = {1518--1522},
	file = {Full Text PDF:/home/tn/Zotero/storage/RF2P5EXI/Schaeffer et al. - 2007 - Checkers Is Solved.pdf:application/pdf;Snapshot:/home/tn/Zotero/storage/SWIAACZL/1518.html:text/html}
}