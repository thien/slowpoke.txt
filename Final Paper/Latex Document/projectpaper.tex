\documentclass[12pt,a4paper]{article}
\usepackage{times}
\usepackage{durhampaper}
\usepackage{harvard}
\usepackage{listings}
\usepackage{color}
\definecolor{lightgray}{gray}{0.9}

\lstset{
    showstringspaces=false,
    basicstyle=\ttfamily,
    keywordstyle=\color{blue},
    commentstyle=\color[grey]{0.6},
    stringstyle=\color[RGB]{255,150,75}
}

\newcommand{\inlinecode}[2]{\colorbox{white}{\fontsize{10}{10}\lstinline[language=#1]$#2$}}

% ---------------------------------
% ---- PACKAGES -------------------
    % package to add wrap figure
    \usepackage{wrapfig}
    % packages to add images
    \usepackage{graphicx}
    % packages for pseudo-code and algorithms
    \usepackage{amsmath}
    \usepackage{algorithm}
    \usepackage[noend]{algpseudocode}
    \makeatletter
    \def\BState{\State\hskip-\ALG@thistlm}
    \makeatother
    % packages to generate neural network graph
    \usepackage{tikz}
    \usetikzlibrary{matrix,chains,positioning,decorations.pathreplacing,arrows}
    % package to add graphs
    \usepackage{pgfplots}
    % stuff for checker board
    \usetikzlibrary{matrix.skeleton}
    \tikzstyle{ball} = [circle, shading=ball, minimum size=1cm]
    \newcommand{\bl}{\node [ball, ball color=black!80!white, draw=black!65!white, thin]{};}
    \newcommand{\wh}{\node [ball, ball color=white] {};}
    \pgfplotsset{compat=1.15}
% ---- END PACKAGES ---------------
% ---------------------------------

    \citationmode{abbr}
    \bibliographystyle{agsm}
    % Titular Information
    \title{A Neuroevolutionary Approach to Draughts Playing Agents}
    \author{}
    \student{T. P. Nguyen}
    \supervisor{S. Dantchev}
    \degree{BSc Computer Science}
    \date{}

    % Start the Document
    % Note that the whole report, including the references, should not be longer than 20 pages in length.  The system will not accept any report longer than 20 pages.  It should be noted that not all the details of the work carried out in the project can be represented in 20 pages.  It is therefore vital that the Project Log book be kept up to date as this will be used as supplementary material when the project paper is marked.  There should be between 10 and 20 referenced papers---references to Web based pages should be less than 10\%.
    \begin{document}
    \maketitle


\begin{abstract}
    % These instructions give you guidelines for preparing the final paper.  DO NOT change any settings, such as margins and font sizes.  Just use this as a template and modify the contents into your final paper.  Do not cite references in the abstract.

    % The abstract must be a Structured Abstract with the headings {\bf Context/Background}, {\bf Aims}, {\bf Method}, {\bf Results}, and {\bf Conclusions}.  This section should not be longer than half of a page, and having no more than one or two sentences under each heading is advised.

    {\bf Background}

    Presently, competitive Draughts AI players are designed to play at a fixed ability. While it has produced very competitive and intelligent players, they require human intervention to improve their performance. This is mainly due to their dependency on pre-defined move databases. Through neuroevolution (Combining Genetic Algorithms and Neural Networks), this issue can be solved by creating a player that can grow in ability over time, by learning to play itself.
    
    {\bf Aims}

    % The objective of this project is to explore how well a neuroevolutionary approach can tackle the game of English Draughts. First, previous historical successes in the field and the components that helped build their systems are studied. Then, the components are looked to see how they can be adapted to suit the objective at hand. 
    
    The aim of this work was to explore whether it is possible to train the weights of a neural network using genetic algorithms. The effectiveness of this approach was measured when it was trained to play Draughts.
    % Whilst the use of evolutionary algorithms and neural networks have been explored independently of one another to create draughts players, the intention is to explore the effectiveness of the combined methods in terms of developing an artificially intelligent draughts playing agent. 

    {\bf Method}

    % The initial population consists of randomly generated AI players, which will play each other to determine the best player out of the population. The performance of championing AI players at every generation of the genetic algorithm are measured against previous champions. Appropriate algorithms are implemented to determine the overall development of the system's ability to play Draughts.
    The method started with designing a feed-forward neural network that took as input, the state of a checker board, and would output an evaluation of the players advantage. This was combined with a tree-search algorithm that chooses the best move from a set of possible outcomes. Finally, the system was used on an existing Draughts framework that provided it the ability to play Draughts. Genetic algorithms were used to adjust the weights of the neural network, with the intention of making the system learn to evaluate checker boards more effectively. 
    
    {\bf Results}

    Overall, the neuroevolutionary approach was shown to learn and improve over time. The net learning rate was positive. However, the wide scope of the adjustments afforded may have impacted the learning rate and was consequently shown to be volatile in some situations. The use of crossovers showed promise, providing a wide range of potential results but on average suggested negligible improvement.

    {\bf Conclusion}

    Under the premise that the tournament requirements are well-defined, neuroevolution can be considered as an option to create a draughts-playing AI. However, the robustness of the system is quite volatile. To counteract this, extra precautions should be taken in producing a crossover mechanism that is both safe but exploratory as this could otherwise negatively impact the learning rate.

\end{abstract}
% Done
\begin{keywords}
    Artificial Intelligence, Machine Learning, Neural Networks, Neuroevolution, Genetic Algorithms, Draughts, Checkers, Crossover, Monte Carlo Tree Search
\end{keywords}
% DONE
\section{Introduction}
    % This section briefly introduces the general project background, the research question you are addressing, and the project objectives.  It should be between 2 to 3 pages in length.  Do not change the font sizes or line spacing in order to put in more text.
    The motivation behind this project is to explore the effectiveness of genetic algorithms to train a neural network. Neural networks can be used to evaluate the performance of two players in a zero-sum game of Draughts. The use of genetic algorithms was used to increase the quality of the neurons. The combined mechanisms would create a Draughts playing agent that, when provided with the option to consider a set of moves, would have the potential to play and learn without human intervention.

    \subsection{Draughts}

    English Draughts (also known as American Checkers) is a popular 2-player board-game played on an 8x8 chess board (also known as a checker board/draughts board). Players choose a colour and begin with 12 pawns of their respective colours, which are placed on the dark-coloured squares of the board. Beginning with the black pawns, each player takes a turn to move a pawn diagonally by one square. In the event that a pawn reaches the opposite side of the board from where the piece originated, it is promoted to a king. Kings have the ability to traverse backwards in the same diagonal motion as pawns. Assuming that there is space upon landing, a piece (pawn or king) also has the option to capture their opponents piece by moving two consecutive diagonal squares, where the opponents piece is situated immediately opposing the players piece. Multiple pieces can be captured consecutively in a single turn if the moves afford the scenario. A player wins by capturing all of their opponent's pieces. A player loses by having all of their pieces captured. A draw occurs when both players agree to draw after a three-fold repetition (where both players take three moves to simulate the same position), or a player has pieces on the board but cannot move any of them. A draw can also be forced when both players fail to capture a piece after a set amount of moves.

    \subsection{Neural Networks}

    Neural Networks (NNs) are non-linear statistical data-modelling tools which link inputs and outputs adaptively in a learning process similar to how the human brain operates. Networks consist of units, described as neurons, and are joined by a set of rules and weights. The neurons are defined with some characteristics and appear in layers. The first layer is defined as the input layer, and the last layer is the output. Layers between the two aforementioned are described as hidden layers. Data is analysed by their propagation through the layers of neurons, where each neuron manipulates the content of the data by some manner. Learning takes place in the form of manipulation on the weights connecting the neurons. This allows it to model complex relationships between inputs and output, and also allows it to find patterns in the data. 

 	\subsection{Genetic Algorithms}

    A subset of evolutionary algorithms in general, Genetic algorithms (GAs) are a group of search techniques used to find exact or approximate solutions to optimisation and search problems. The methodology is inspired by Charles Darwin's evolutionism theory; individuals are created based on the improvement of their ancestors' genetic information (genome). Genomes are analogical to a data structure, most commonly a 1D array. Evolutionary algorithms typically consist of a population of agents, a tournament selection process (also known as a fitness function), and the introduction of probabilistic mutation on genomes. This in turn allows evolutionary algorithms to create genomes that increase in quality over time with regards to the tournament. Genetic algorithms distinguish themselves from evolutionary algorithms through their use of algorithmic crossover mechanisms, where two parent genomes are combined together to create offspring. The use of evolutionary algorithms to train neural networks is described as a type of machine learning called Neuroevolution.

    % \subsection{Deliverables}

    % D. Deliverables
    % The research question that we ask is, 'Can we improve distributed clustering methods, and implement a new algorithm, by modularising and extending existing techniques, and considering object and cluster scope?' To achieve this we set project deliverables categorised into basic, medium and advanced objectives. The basic objective was to specify a simple object clustering method, and then implement the method in a simulation program. The simulation program was to have a simple user interface for users to change some settings such as the number of objects in the virtual environment, and whether the object distribution is even or skewed. The purpose of the basic objective was to create a foundation for future work; our algorithm needed to be incremental and upgradable, and the simulation program needed to be able to compare and process multiple algorithms. We achieved the basic objective with an implementation of k-means in a high performance hardware accelerated multithreaded simulation environment, programmed using OpenGL and C++. This tool has the ability to render millions of concurrent dynamic objects and paint them in any distribution scenario using an airbrush. The airbrush has the additional capabilities of being able to change object behaviour between one of six different types, and change the size and density of the paint area. The intermediate objectives were to enhance the clustering method to handle both non- Euclidean and Euclidean metrics. Non-Euclidean metrics are important in game environments as they take into account players who are interacting with distant players, such as friends communicating via a messaging system. Our k-means foundation needed to be extended to handle non-Euclidean metrics. Unfortunately, the very nature of k-means is to cluster based on the distance to a centroid; which only produces sphere-shaped clusterings. However, OMBG achieves the intermediate objective by not clustering based on distance, instead it groups data by a winner-take-all principle between an object relationship function. With this approach, it is possible to alter the weightings in the relationship function such that objects form groups with nearby objects of the same behaviour more frequently than with objects of different behaviours. We also show how it would be possible to form fuzzy clusterings between object behaviours and local groups. The advanced objectives were to enhance the clustering method by considering object and cluster scope. This was then to be evaluated accordingly to see whether these considerations could bring an improvement to the field. In order to carry out evaluation, it was decided that a selection of the best existing algorithms would be implemented. It was also decided that we would evaluate the algorithms using different dynamic object 3behaviours, to represent various playing styles in games. We achieved the advanced objectives by extending the method used in TKM, which is to assign each object a circular threshold. We provided each object with a set of primitive shapes acting as their scope. The object then contains two variables; its shape threshold and current location. The object only issues an update to the server when its current location is not within its shape threshold. This greatly reduced the network overhead of OMBG to a fraction of what is used in TKM, giving excellent results.
    \subsection{Aims}

    % Research Question
    Whilst the use of evolutionary algorithms and neural networks have been explored independently of one another to create draughts players, the intention is to explore the effectiveness of the combined methods in developing an artificially intelligent draughts playing agent. 

    \subsubsection{Minimum Objective}
        % The minimum objective of this project was to design and build a programme that would display a polygonal mesh of triangles, with help of a mesh data structure. The basic mesh data structure would have to enable future features that the next objectives would require.
        The minimum objective was to implement a feed-forward neural network, alongside a Draughts game interface for the network to evaluate on. This would need to be paired with a tree-search algorithm that determined the move to choose at some given state of the game. Finally, a genetic algorithm needed to be implemented that interacted with the neural network, which allowed the possibility to learn over time. This was achieved by having each component implemented manually in Python, with a heavy dependency on the NumPy library to maximise performance of the algorithms.

    \subsubsection{Intermediate Objective}
        % The intermediate objective of this project was to create and incorporate the algorithms for the calculation of triangle surface normals, vertex normals and gradient (following our definition) in the initial programme, and enable visualisation of the “interesting” gradient changes and vertex normals on the mesh displayed by the programme.
        The intermediate objective involved the implementation of an interface to play against the trained result, and the implementation of a Monte-Carlo Tree-Search (MCTS) algorithm. This allowed the system to think ahead, which would further allow it to determine better quality moves. Also, as the genetic algorithm included a tournament mechanism which would play games simultaneously, this needed to be implemented to run in parallel. The implementation of the tournament mechanism was made possible via the use of a round robin approach and the use of the multiprocessing Python library. MCTS was successfully implemented manually, which improved the quality of the moves the system made.
        
    \subsubsection{Advanced Objective}
        % The advanced objective of this project was to evaluate the speed of the algorithms and compare it to the time complexity that we obtained from theoretical analysis of the algorithms. The final sections of this paper will show the results acquired and the analysis and evaluation of the work we carried out. 
        The advanced objective consisted of producing a system showing indications that it was learning over time. Measurements were taken to determine whether the genetic algorithm improved the neural network, and which properties of the genetic algorithm made it possible. Measurements were taken at every generation that calculates the distribution of the different components, which is explained in more detail in Results.
    
    % To template the document, the remaining sections of the paper discusses related and relevant work, the proposed solution, results and an evaluation of the project. The paper is then wrapped up with a conclusion, discussing the implications of the project and potential future work.

% TODO
\section{Related Work}
    % This section presents a survey of existing work on the problems that this project addresses. it should be between 2 to 4 pages in length.  The rest of this section shows the formats of subsections as well as some general formatting information for tables, figures, references and equations.

    % Intro
        Draughts has been historically used as a testing ground for artificial intelligence and computational capabilities since the early introduction of computers. This was due to the relatively vast search space, making the game difficult to brute-force, until recently. Research has shown that it is possible in theory to solve the game, but the computational performance was not available to prove the method at the time of discovery \cite{schaeffer_solving_1996}. The same researchers, however, reinvestigated the research a decade afterwards, eventually proving the game to be \emph{weakly-solved}. \cite{schaeffer_checkers_2007}. Weakly-solvable is defined such that ``the game-theoretic value for the initial position has been determined, and that a strategy exists for achieving the game-theoretic value from the opening position.'' \cite{allis_searching_1994} However, the premise of this paper was not necessarily to solve the game itself, but to rather see whether neuroevolution was a method that, when measured via its ability to play the game, shows that it can evolve in capability over time.

        % Research in artificial intelligence alone has been active since, targeting games with a higher move space, notably Chess and, more recently, Go. This section will summarise specific and notable techniques relevant to the discussion of the task at hand.

    % DONE
    \subsection{Arthur Samuel}
        An early design of machine learning to play Draughts by Samuel \cite{samuel_studies_1959} revolved around the continuous improvement of a linear combination of heuristics. These heuristics represented different strategies players considered when playing Draughts, and when combined, acted as an evaluator function for a state of the checker board. Some of these heuristics included the number of pieces on a  side (such that one side had a particular advantage given that they had more pieces), and the number of pieces positioned along the central diagonal. His evolutionary strategy consisted of agents, each of which represented coefficient multipliers for the heuristics (initially random), and new agents are formed via the manipulation of its successor's coefficients with subtle mutations.
        
        % The weights were then trained by the algorithm playing itself in a form of genetic programming. 
        
        % talk about minimax here.
        Samuel's game tree-search algorithm primarily revolved around an early concept of the mini-max algorithm. This algorithm took as input the present state of the checker board, expands the move-tree to some ply-depth, and returns the best move to choose. A ply refers to the number of moves in advance. Ply-depth refers to the depth of the tree based on the number of moves in advance. Mini-max can be described using the following instructions:
        
        \begin{enumerate}
            \item A game tree is expanded from the root node, where the root node represented the current state of the game.
            \item Each node represents a state of a checker board. Child nodes represents a future state of the checker board. Child nodes can only be produced from the parent node; i.e. a move from a parent node will produce a new state represented in the child node.
            \item Child nodes are expanded until some depth was reached. From there, an evaluation function is used to calculate the value of the leaf nodes at this depth. This evaluation function would produce a value that indicates a player's advantage at that state.
            \item The best potential position for a given player is deduced from the nodes at this stage, which is quantified with some value.
            \item The best value propagates upwards until it reaches to a child of the root node. From here, the best move can be deduced, corresponding to the state with the highest value.
        \end{enumerate}

        There are inherent trade-offs to this tree-search algorithm, especially with utilising higher ply counts; asymptotic complexity of mini-max is exponential; it being $O(x^y)$ with $x$ being the branching factor, and $y$ being depth. Alpha-Beta pruning, an optimisation to mini-max, consisted of pruning nodes which are already recognised to be disadvantageous from the tree, reducing the number of evaluations needed. Whilst Alpha-beta pruning did improve the running time of the algorithm (having a best case of $O(\sqrt{x^y})$, it did not fundamentally reduce the worst case complexity of the algorithm (remaining in $O(x^y)$). This exponential growth is the Achilles' heel to the mini-max approach, which made it less attractive when considering tree-search algorithms for the system.

        A later investigation by a checkers magazine evaluated Samuel's player to perform at below Class B \cite{schaeffer_one_1997,fogel_evolving_2000}; (Class B being categorised with an ELO range of [1600-1799]). This implied that it played at a performance that can be seen at club level players. \cite{elo_rating_2008} However, the performance of the system is handicapped by the quality of the heuristics Samuel devised; i.e. the system can only be as good as whatever Samuel considered as a heuristic to consider when deciding what move to make.

    % DONE
    \subsection{Blondie24}
        The idea of evolving neural networks to evaluate board-states was based on the success Chellapilla and Fogel had in evolving their own Draughts playing neural networks. Their method used Samuel's system as a foundation and modified the evaluation method, which used feed-forward neural networks instead of a linear combination. Their resulting AI, Blondie24, was taught to play without priori knowledge in a similar vein to Samuel's system. \cite{chellapilla_evolving_1999}

        Blondie24 used an evolutionary algorithm, using a population of fifteen agents $i=1,\dots,15$, each of which consisted of two properties. The first property were weights of the neural network (which are initially random), and the second property consisted of a self-adaptive parameter vector, $\tau_i$, which determined how random the mutations are when the agent $i$ evolved.
    
        In each generation, every agent played each other in a round robin style tournament. Agents played an equal number of games, and successors were chosen based on their performance in the tournament. The best agents were chosen to mutate themselves, which produced new agents in the chance that they performed better than their predecessors. Mutation was performed using the formula $\tau_i(j) = \tau_i(j) \cdot e^{\mu}$, where $j=1,...,N_w$ represented individual weights of the neural network $w$, and $\mu$ represented a random decimal in the range of $[0-1]$. Like Samuel's program, Blondie24 also used Mini-Max with alpha-beta pruning.

        However, crossover mechanisms were not considered (and therefore cannot be classified as using genetic algorithms, hence their classification of evolutionary algorithms). Blondie24's neural network structure consisted of a $\left\{ 32,40,10,1 \right\}$ set, such that the input layer consisted of 32 nodes, and 1 output node. The output node represented a classifier value in the range of [-1, 1], which represented a particular advantage for a given side of the board. One issue with this particular method was that spatial awareness could not be immediately interpreted as it took an immediate input of the positions on the board. This would make it inherently more difficult for the neural network to generate heuristics based on spatial awareness.

        The motive was not to repeat the work produced by Chellapilla and Fogel, but sections of the best parts of their system were extracted and manipulated to fit the solution described in this paper, such as the neural network structure and the mutation formula. 

    \subsection{Post Blondie24}

        Blondie24 was influential in the field of neuroevolution during its time, which led to various studies on the theory behind their success. A notable example was the series of publications by Al-Khateeb and Kendall, where they explored in detail the different features that comprise Blondie24. One of their studies concerning the effects of the ply depth, concluded that having a higher ply unsurprisingly increases the overall performance of the player, and that agents trained at a higher ply performed significantly worse when playing at a lower ply. More importantly, the results in the paper suggested that training with a 4-ply produced the best value function to start with during the learning phase, and then extending the ply for evaluations. \cite{al-khateeb_effect_2012} This was considered in the final implementation, where the system was tested with ply depths around this value. 
        Their study on the influence of tournaments in Blondie24 concluded that the league structure (commonly seen in standard tournaments) seemed more appropriate to use as opposed to the round robin tournament. \cite{al-khateeb_introducing_2009} Round Robin tournaments refers to having each participant play every other participant. This was however more difficult to set up in parallel, which was a large contributing factor in the runtime of the system.
        
        % Alternative contemporary machine learning techniques have been exercised against Draughts. Franken et al. (2003) used draughtsboards as an evaluator for their rendition of an AI through Particle Swarm Optimisations.[14] 
        % Within research in the game of Draughts alone, A research by Toledo-Marin et al. looked at studies on attacking strategies using Draughts as a reference (which however can be applied to any other zero-sum game), where they mathematically prove that for an offensive player, maximising the offensive improves their probability to win.\cite{toledo-marin_is_2016} This could be considered to guide the system to lean towards attacking moves, which could potentially accelerate the learning rate of the system.

        % Stanford's SVM method of improving Blondie
        % Cobbe, et al. improves Blondie24 by using an evaluation function that consists of Support Vector Machines, which achieves locally optimal performance in approximately a third as many generations than its typical evolution. \cite{cobbe_accelerating_nodate} 
        
    % TODO: fix write up of everything, talk about monte-carlo
    % \subsection{Neural Networks}
    %     % need to mention regular gradient based learning
    %     Lai's Giraffe \cite{lai_giraffe:_2015} program used a deep reinforcement learning technique to play Chess to an advanced level in a relatively quick time-period. Giraffe's decision function involved using Temporal Difference Learning (TD-Leaf), an improvement on MiniMax, by Baxter et al. \cite{baxter_tdleaflambda_1999} However, Giraffe has been trained on previously played games from grandmaster tournaments as a reference, and was not entirely self-taught.
        % Lai then moved on to join DeepMind, 

        % talk about monte-carlo

    % TODO: fix writeup of everything
    \subsection{Return of "Genetic" Algorithms}

        Since then, there has been an increasing number of published papers describing the promising utility of GAs and NNs in general. GAs have been shown to evolve weights for a convolutional-NN in a classification task better than back-propagation, in nearly all situations, other than when the number of generations generated for NNs are small. \cite{perez_apply_nodate} Another research showed GAs ability to compute optimal structures for deep NNs, using an image recognition dataset as the benchmark for competitiveness against the networks. \cite{xie_genetic_2017}
        
        % Kusiak et al. (2007) have devised the use of pure genetic algorithms as the premise to evaluate the board. [16] they describe the use of 25 heuristics that, when used in unison, can determine the evaluation of a given board. They continue to go further on and specify that various combinations of heuristics, when used in different stages of the game, prove to be the most promising of outcomes.

        % Uber's revival of genetic algorithms and safe mutations
        However, one recent paper by Clune et al. on behalf of Uber Labs returned to the use of GAs as the premise to train especially deep convolutional networks with success. \cite{such_deep_2017} Their neuroevolutionary system was used to play popular Atari games on the OpenAI framework, and was found to perform as well as other contemporary evolution strategies and reinforcement learning algorithms. 

        The success of the program was based on their system's ability to perform safe mutations, a technique that revolved around measuring the sensitivity of the network to changes on some of the weights. \cite{lehman_safe_2017} This afforded the system to evolve deep NNs (In this particular case, containing over 100 layers and over four million weights and biases), in a manner that allowed random but safe and exploratory searches. Their safe mutation exploited the knowledge of the NN structure, by retaining sampled experiences of the network (inputs that were fed into the network and its outputs). This allowed the weights to be manipulated by comparing their significance in the neural network with the output, by using the sampled experiences. 

        While they do not consider the use of crossover techniques in their paper,  they briefly mention that it would very well be possible to produce a safe crossover method that exploits structures of a NN, inspired by their research produced on safe mutations.

        Unfortunately, it was unrealistic to apply the same techniques described in their paper into the proposed system. The machine used as the base (consisting of hundreds to thousands of cores) was of a magnitude greater than what was available for the task at hand, but a more simplistic and adapted model can be produced using similar ideas. 
        
        A popular trend within the contemporary research in this section was the shared absence of the use of crossover algorithms. One paper describes the use of different crossover algorithms on NNs having no clear advantage over each other, in comparison to a uniform crossover (uniform crossovers are also commonly defined as the textbook crossover) which disrupted the overall performance of the NNs. This could be due to building blocks being disrupted as chunks of the sequential weights are swapped out of each other \cite{emmanouilidis_comparison_2000}. It was important to consider this when devising a crossover method, such that the effect it has on the NN should be subtle enough that it should pose similar results to the network prior to crossovers.

% TODO: Write up everything..
\section{Solution}

    % This section presents the solutions to the problems in detail.  The design and implementation details should all be placed in this section.  You may create a number of subsections, each focussing on one issue.  

    % This section should be between 4 to 7 pages in length.
        
    This section describes the implementation process of the system. Implementing the checker board interface is not included in the discussion as it is not one of the objectives. However, the sections below are considered under the premise that the interface was built prior to their development.

    \subsection{Evaluation Function}
        
        % \begin{wrapfigure}{r}{0.37 \textwidth}
            %     % \begin{center}
            %     \fontsize{8}{11}
            %     % \vspace{-40pt}
            %     \centering
            %         \caption{The indexes of the 32 pieces of the input layer are the immediate values of the positions on the board. \label{boardarray}}
            %         \vspace{5pt}
            %         \begin{tikzpicture}
            %         \color{white}
            %         \matrix (m) [matrix of nodes, nodes in empty cells, label skeleton, nodes={minimum size = 0.4cm}] {
            %         & 31 &&30&&29&&28 \\
            %         27 &&26&&25&&24&&\\
            %         &23&&22&&21&&20\\   
            %             19&&18&&17&&16&&\\
            %         &15&&14&&13&&12\\
            %         11&&10&&9&&8&&\\
            %         &7&&6&&5&&4\\
            %         3&&2&&1&&0\\
            %         % \bl &     & \bl &     &     &     & \bl &     \\
            %         %     &     &     & \bl &     &     &     &     \\
            %         % \wh &     &     &     &     &     &     &     \\
            %         %     &     &     & \wh &     & \wh &     & \wh \\
            %         % \wh &     & \wh &     & \wh &     & \wh &     \\
            %         %     & \wh &     & \wh &     & \wh &     & \wh \\
            %         };
            %         \foreach \row in {1, ..., 8} {
            %         \foreach \col in {1, ..., 8} {
            %             \pgfmathparse{Mod(\row + \col, 2) ? "black" : "white"}
            %             \colorlet{squarebg}{\pgfmathresult}
            %             \fitandstyle[background]{(m-cell-\row-\col)}{fill = squarebg}
            %         }
            %         }
            %         \end{tikzpicture}
            %     % \end{center}
            %     \vspace{-20pt}
            % \end{wrapfigure}
            % % talk about how it works
            % % talk about the preprocessing
        
        A feed-forward multilayer perceptron neural network was used to evaluate the board. The network contained 4 layers; the input layer consists of 91 nodes, with the output node having 1. Hidden layers had 40 and 10 nodes respectively, and the dimensions are based on the success of Blondie24's model \cite{chellapilla_evolving_1999}. Figure \ref{nnmodel} shows a visual representation of the network structure.

        % Neural Net figure
        \begin{figure}[ht!]
            \centering
            \caption{The chosen neural network model. The preprocessed values of the checker board are used as input. An output was produced after propagation that ranged from [-1,1]. \label{nnmodel}}
            \vspace{10pt}
            \tikzset{%
                every neuron/.style={
                    circle,
                    draw,
                    minimum size=0.6cm
                    },
                neuron missing/.style={
                    draw=none, 
                    scale=4,
                    text height=0.2cm,
                    execute at begin node=\color{black}$\vdots$
                },
            }

            \begin{tikzpicture}[x=1.5cm, y=1.1cm, >=stealth]

                % draw nodes
                    % % input values
                    % \foreach \m/\l [count=\y] in {1,2,3,missing,4}
                    % 	\node [every neuron/.try, neuron \m/.try] (input-\m) at (-0.5,2.5-\y) {};

                    % input layer
                    \foreach \m/\l [count=\y] in {1,2,3,missing,4}
                    \node [every neuron/.try, neuron \m/.try] (input-\m) at (0,2.5-\y) {};
                    
                    % layer 1
                    \foreach \m [count=\y] in {1,missing,2}
                    \node [every neuron/.try, neuron \m/.try ] (hidden1-\m) at (2,2-\y*1.25) {};

                    % layer 2
                    \foreach \m [count=\y] in {1,missing,2}
                    \node [every neuron/.try, neuron \m/.try ] (hidden2-\m) at (4,2-\y*1.25) {};

                    % output layer
                    \foreach \m [count=\y] in {1}
                    \node [every neuron/.try, neuron \m/.try ] (output-\m) at (6,0.5-\y) {};

                % input layer
                \foreach \l [count=\i] in {1,2,3,91}
                    \draw [<-] (input-\i) -- ++(-1,0)
                    node [above, midway] {$I_{\l}$};

                % hidden 1
                \foreach \l [count=\i] in {{(1,1)},{(1,40)}}
                    \node [above] at (hidden1-\i.north) {$H_{\l}$};

                % hidden 2
                \foreach \l [count=\i] in {(2,1),(2,10)}
                    \node [above] at (hidden2-\i.north) {$H_{\l}$};

                % output layer
                \foreach \l [count=\i] in {1}
                    \draw [->] (output-\i) -- ++(1,0)
                    node [above, midway] {$O_\l$};

                % lines from input to hidden 1
                \foreach \i in {1,...,4}
                    \foreach \j in {1,...,2}
                        \draw [->] (input-\i) -- (hidden1-\j);
                
                % lines from hidden 1 to hidden 2
                \foreach \i in {1,...,2}
                \foreach \j in {1,...,2}
                    \draw [->] (hidden1-\i) -- (hidden2-\j);

                % lines from hidden 2 to output
                \foreach \i in {1,...,2}
                    \foreach \j in {1}
                        \draw [->] (hidden2-\i) -- (output-\j);

                % headings
                \foreach \l [count=\x from 0] in {Input, Hidden, Hidden, Output}
                    \node [align=center, above] at (\x*2,2) {\l \\ layer};

            \end{tikzpicture}
            % \vspace{-25pt}
        \end{figure}
        

        The inputs of the network were a pre-processed 1D array of the checker board. Pre-processing the input began with retrieving the positions of the pieces on the checker board in the form of an 1D array and calculating all possible individual subsquares of the board. A subsquare is a square subsection of the board. The subsquares kernel size range from a 3x3 to 8x8. Figure \ref{subsquares} visualises the retrieval of the subsquares. With the kernels, the number of pieces in each subsquare are summed up. A new array was formed, comprising of the sums of the different kernels that make the board. The resulting array (representing 91 entries) is the preprocessed input, which was fed to the neural network.
        
        To create values for the subsquares, the black pawns were weighed with a value of $v=1$, and white pawns as $-v$. It is commonly understood that a king is worth more than a pawn piece, but its precise value advantage is disputed. This could have been decided by the system such that it evolved to understand that a king was eventually worth more than the pawn. however, for the sake of simplicity, a king's piece value was weighted at $1.5v$. This value was chosen based on the results produced by Al-Khateeb's research on piece values, where evolution on the piece differentials plateaued towards the value $1.5$. \cite{al-khateeb_importance_2010}.

        \begin{figure}[!ht]
            \centering
            \includegraphics[width=110mm]{images/subsquares.pdf}
            \caption{A diagram visualising the processing of subsquares on board (represented as the 1D array $x$) to create the array $\widehat{x}$. Each index in $x$ represents a position on the board, such that each item in $x$ represents a value of the piece on the board. $\alpha$, $\beta$ and $\gamma$ are examples of 3x3 and 4x4 subsections. \label{subsquares}}
        \end{figure}

        % Talk about the activation function
        Weights $w$ of a neuron are multiplied by their input value $I$, summed with their bias $B$, and are passed through an activation function $O = f((I* w) + B)$, to become an input for another neuron. Activation functions, when visualised, usually have a sigmoid curve, but may also take the form of other shapes. Common characteristics of activation functions include values which are monotonically increasing, continuous, differentiable and bounded. 
        % how does it work?

        % Tanh Diagram
        \begin{wrapfigure}{r}{0.3\textwidth}
            \centering
            \begin{tikzpicture}
                \fontsize{6}{8}
                \begin{axis}[
                    legend pos=north west,
                    axis x line=middle,
                    axis y line=middle,
                    x tick label style={/pgf/number format/fixed,
                                        /pgf/number format/fixed zerofill,
                                        /pgf/number format/precision=1},
                    y tick label style={/pgf/number format/fixed,
                                        /pgf/number format/fixed zerofill,
                                        /pgf/number format/precision=1},
                    grid = major,
                    width=55mm,
                    height=4cm,
                    grid style={dashed, gray!30},
                    xmin=-5,     % start the diagram at this x-coordinate
                    xmax= 5,    % end   the diagram at this x-coordinate
                    ymin= -1,     % start the diagram at this y-coordinate
                    ymax= 1,   % end   the diagram at this y-coordinate
                    %axis background/.style={fill=white},
                    xlabel=x,
                    ylabel=y,
                    tick align=outside,
                    enlargelimits=false]
                % plot the stirling-formulae
                \addplot[domain=-5:5, red, ultra thick,samples=500] {2*(1/(1+e^-x))-1};
                % \addlegendentry{$f(x)=\frac{1}{1+e^{-5x}}$}
                \end{axis}
            \end{tikzpicture}
            \caption{Graph of tanh function $f(x)={\frac{2}{1+e^{-x}}- 1}$. \label{sigmoid}}
            \vspace{-10pt}
            \end{wrapfigure}

        The choice for the activation function is tanh (a type of sigmoidal function), shown in figure \ref{sigmoid}. 
        % There exists inherent issues related to the properties of their derivatives, commonly known as the missing gradient problem. This is an issue  \cite{nair_rectified_2010}, described as the missing gradient problem. However, since this issue is related to the properties of a gradient based learning method and not through a stochastic learning method (of which genetic algorithms are), this is not a concern for the project. 
        The use of tanh facilitated the ability to simulate a zero-sum formula (for a range of [-1 to 1]) due to its symmetric properties around the origin.
        
        Results are cached, such that evaluations are saved and then can be recalled immediately, in the event that the same input was requested to be  evaluated again. This showed  varying levels of time improvement based on the ply. As the ply increased, the size of the state space increased, and therefore the chances of recalling the same set of inputs decreased. The cache was also used for our safe mutations algorithm, described later on in Section \ref{coefficient_mutation}.

        The neural network was implemented manually via NumPy to allow direct access to the weights, allowing the genetic algorithm access to manipulate them.
    
    % DONE
    \subsection{Tree-Search Algorithm}
        Initially, the tree-search algorithm implemented was the same mini-max approach as in Samuel's program \cite{samuel_studies_1959}, but was then progressed to a modified Monte-Carlo Tree Search (MCTS). A basic MCTS differs from mini-max where future moves are probabilistically played to the end of the game. It acts as a sampling of possible outcomes, and does not depend on an evaluation function at all. A survey found that MCTS can dramatically outperform mini-max based search engines in games where evaluation functions are otherwise difficult to obtain \cite{browne_survey_2012}. MCTS was played in rounds which consist of four operations:

        \begin{itemize}
            \item Selection: A selection strategy was played recursively until some position was reached. 
            \item Play-Out: A simulated game was played. Again, a basic form of MCTS would play until an end-game was reached.
            \item Expansion: A node was added to the tree.
            \item Back-propagation: The result of the game was back-propagated in the tree of moves.
        \end{itemize}

        These rounds are repeated until a certain period of time was met or a certain number of rounds are computed. Once finished, a node was chosen based on its total probability of reaching a winning outcome.

        \begin{figure}[!ht]
            \centering
            \includegraphics[width=90mm]{images/montecarlo.png}
            \caption{A diagram describing the MCTS process.}
        \end{figure}

        MCTS-EPT (or MCTS with early playout termination) modifies the MCTS random play-out approach. Instead of allowing the random moves play to the end of the game, the number of moves traversed are capped and an evaluation function (which was the neural network) was applied at that future state instead. \cite{lorentz_using_2016} The model was then adapted such that when the number of possible moves was less than 4, the ply would extend by the number of moves. This reduced the need to depend on random playouts of moves to the end state, and ensured that the quality of the moves was contingent on the evaluation function. For the results, we use the 1,3 and 6 ply to determine whether the system played consistently based on their depth. As opposed to using a time threshold, the number of MCTS rounds $r$ was based on the ply $p$, such that $r=300p$. This allowed the system to process moves in a manner that the performance of the system was hardware-independent, as faster machines could have processed more rounds within the same fixed set of time.

        % talk about the fact that this is a probabilistic strategy

    \subsection{Genetic Algorithms}
        % talk about generation of population
        % talk about tournmanent
        % talk about 
        The genetic algorithm (GA) was the premise of the system's learning strategy. GAs are used to train the neural network. The various algorithms that form the collection of GA strategies are discussed below. For the system, a population size of 15 was chosen, due to the restraints on available computational performance.

        \subsubsection{Population Generation} \label{population_generation}
            % Introduce what its for
            In genetic algorithms, the population serves as a base that allows agents in the pool to play each other. Every generation has its own population of agents, and the population size was consistent across the generations. The initial population consisted of randomly generated weights and biases of the neural network, with values from [-1,1]. 
            
            For a population size of 15, the next generation was created using the best five agents from the current generation (discussed in {\it{\ref{tournament_selection} Tournament Selection}}.) They continued to play in the next generation; this strategy is described as elitism. The next eight players were generated through the use of crossover strategies (see {\it{\ref{crossover_strategy} Crossover Strategy}}). The weights of the $1^{st}$ and $2^{nd}$ place agents are used as inputs to the crossover strategy and generated 4 off-springs. Two were reciprocal crossover representations of each other, and the other two were directly mutated from the parents themselves. Another 4 children are created using the same strategy, with the 2nd and 3rd agent's weights. The remaining two were direct mutations of the 4th and 5th place agents.

        \subsubsection{Tournament Selection} \label{tournament_selection}

            To sort the quality of the players in a population, a tournament selection process was deduced. This allowed the best players who would continue to play in the next generation to be chosen.

            Each agent in the population plays a minimum of 5 games as Black, against randomly selected opponents. Each game lasts a maximum of 100 moves from both players. If a winner was not deduced at this stage, a draw was called. Draws were also called when there was a three-fold move repetition from both players. A win was worth $2$ points, a draw being none and a loss being $-1$ point. Both the agent and its opponent received a score from the game.
            Scores were then tallied up at the end of the tournament. Players were sorted by the number of points they scored. The best players had the highest number of points.
  
            All the games were run in an `embarrassingly parallel' fashion through the use of Python's multiprocessing library. Whilst the league structure was found as the better approach \cite{al-khateeb_introducing_2009} it would increase the overall running time as the initialisation of some games are dependent on the outcomes of other games. This would lead to a longer overall running time even though fewer games are played, whereas the embarrassingly parallel method allowed every game to play simultaneously.
            
        \subsubsection{Coefficient Mutation} \label{coefficient_mutation}

            \begin{figure}
            \begin{equation}
                % \caption{The mutation formula. \label{mutation}}
                w_n = w_p + \frac{m}{\sqrt{2 * \sqrt{K} }}
            \end{equation}
            \caption{The mutation formula, where $w_p$ is the current weight, $K$ represents the number of weights and biases in the neural network, and $m$ representing a random floating-point in the range of [-1,1].\label{mutation}}
            \end{figure}

            To create variation between agents and their offspring, statistical anomalies were made through the use of mutations. This was used as one of the learning mechanisms that helped change the deciding factors of the neural network. Weights and biases of an agent's neural network would increment by a random value that was created using the equation in Figure \ref{mutation}.

            Similarly to the activation function,
            % (in \ref{sigmoid})
            the weights would have a soft maximum of [-1, 1]. This consequently meant that the mutation was not controlled, but rather dependent on the number of weights in the system. The more weights in the network implies a less significant mutation. Safe mutation \cite{lehman_safe_2017} was also used. A subset of historical neural network calculations was used as a premise to guide the mutation. The method is as follows:
             
             \begin{enumerate}
             \item With the current set of weights $w$, choose a subset of pre-calculated input and output tuples $\phi_{w} = (I, \lambda_{I,w})$ where $I$ represents the input values of the network, and $\lambda_{I,w}$ as the output.
             \item Generate a perturbation $y$ of the weights calculated using the formula in Figure \ref{mutation}. Use $y$ as a basis of a neural network. 
              \item Using the inputs $I$ in $\phi_{w}$, calculate a new set of input and output tuples $\phi_{y} = (I, \lambda_{I,y})$.
              \item Calculate the divergence $\delta = \lambda_{I,y} /\ \lambda_{I,w}.$ 
              \item Calculate the quality of $y$ based on the number of inequalities $\delta \geq 1$.
             \item Repeat parts 2-5 some $n$ times, keeping note of the best $y$. 
             \end{enumerate}

            The intuition behind this was that if $\phi_{w}$ was a subset of moves that led to a winning game, then the manipulation of the weights was left open as long as it can replicate the performance of winning games.
            
        \subsubsection{Crossover Strategy} \label{crossover_strategy}
      
           \begin{figure}[!ht]
                \centering
                \includegraphics[width=80mm]{images/crossover.png}
                \caption{A diagram visualising the crossover process.\label{crossoverpic}}
            \end{figure}

            The distinct learning mechanism unique to genetic algorithms was the use of crossovers. This combines the traits that build two parent agents to create children. This scenario the weights and biases of the parent's neural networks was used.

            Two off-springs were created from a pair of parents, with each offspring being the reciprocal crossover of each other. 
            % The weights of both parents (now each treated as a 1D array of coefficients), are divided contingent on the number of weights and biases for a given layer. Each layer should be treated separately to reduce the potential dependency on a purely randomly generated neural network. For each set of weights in a given layer, Algorithm \ref{crossover} describes the crossover process in pseudocode.
            A random node from the dominant parent was chosen. A dominant parent was the agent who gained more points in the tournament round than the other (who would be the submissive parent). If a dominant parent could not be deduced, it was randomly chosen. 

            Once a node was chosen, a subset of weights and biases that were fed into the chosen node were swapped with the submissive parent's values at the same positions. As values in the weights can range from -1 to 1, dominant input weights are values that are greater than 0.75 or less than -0.75. Figure \ref{crossoverpic} visualises the process. In the diagram, Network A and B are parents, creating reciprocal offspring networks C and D. A was the dominant network, and A's node with the red inputs was chosen. The weights and biases of the red inputs were swapped with the values at the same position in network B. Network C was formed with Network B's dominant weights, with the rest being composed from A. Network D was the reciprocal, having Network A's dominant weights and the rest from B.

            The crossover should create a subtle modification to a neural network, by swapping a small subset of weights and biases at a time. Having a more dramatic crossover could potentially change the structure of the network flow, reducing its effectiveness. 
            
            % Talk about how this is not commonly used.

    \subsection{Testing}

        Each component of the system, such as the neural network, MCTS, and genetic algorithms, were built in their own classes. This helped to achieve two things; unit testing, and modularisation. The use of unit testing (performed per class) helped to verify that individual components worked as expected, especially with a project of this scale. Components that are shown to work are then added to the system. Training the system required time (discussed further in Figure \ref{cpu_table}), and realising errors was a notably labourious task. Modularisation of the code allowed for quick deployment of new features, such as migrating tree-search algorithms.
    
    \subsection{Evaluation Method}
        % this stuff is measured in the results
        The success of the research question was measured with various statistics. 

        At the end of a given generation, the growth of performance was measured by using the generation's champion. When a new champion was generated, it was played against the previous 5 champions from earlier generations. 6 games were played for each previous champion, with 3 being as Black, and 3 being White. A mean score was calculated from those 6 games. The overall performance of the current champion was the mean of the means of the 5 sets. This value would represent the learning value. A cumulative chart of the learning values represent the learning rate.
        
        A positive learning value can be deduced when the mean of means were greater than 0. Point scores for the champion games were measured by {1,0,-1} where a Win counts as 1 point, 0 for a draw, and -1 for a loss. The weights were scaled differently to the regular tournament in order to portray an even distribution of the learning rate.

        The genomic properties that were involved in creating the champion were also measured. This was combined with the score the champion produced from the learning value to determine the likelihood of creating a champion based on the genomic properties, and the score distribution it was likely to produce.
    
        The end player, representing the champion at the last generation, was used to create comparison games against its ancestors. This player was to be first tested against an agent who was choosing random moves to verify that the system was not playing in some random fashion. Measurements from these games, alongside its champion scores were used as evidence to determine whether the agent was learning over time, thus leadning to an answer to the research question.

        Other measurements include the number of moves played in a game, and the CPU time utilised for system training. These statistics measured the success of the tree search implementation, and its relationship with the CPU timing. The two statistics were then correlated against each other to determine any spurious relationships between the two.

       
    \subsection{Tools}
    % Description of tools used
        The system was built using Python 3.6 and the \inlinecode{Python}{NumPy} library. Initial runs operated on a 1 and 2-ply load to determine the stability of the system. As the memory usage of the system was negligible, it was not considered when choosing hardware. This was run on a Linux (Ubuntu) machine containing a 4-core Intel i5 6600u processor. Development and debugging was also handled on this machine. Once testing has shown to be stable, the system was then configured to run on Durham's MIRA distributed system (Debian). MIRA contains 4 Intel Xeon E7-8860 CPUs (Each CPU contains 16 physical cores running in 2.2Ghz, with hyper-threading). This comes to a total of 64 Cores, and 128 threads. The \inlinecode{Python}{multiprocessing} library was used to run the tournaments in parallel. To interpret the results, charts were generated at the end of every generation using \inlinecode{Python}{pyplotlib}. A consistent SSH connection was used to ensure the simulations were kept alive for the duration of the simulation on MIRA. Once training was complete, performance measurements was also conducted on MIRA, alongside other statistical evaluations to reduce computational time. The end champion was then transferred to the initial testing machine to be played against human input.


        %  Reducing the time involved in debugging allowed for more time for training.

    % Issues
    \subsection{Issues}
    %   discussion and testing
        Verifying that the moves were properly chosen was difficult and slow, as games needed to be played out first, with the quality assurance handled afterwards. The main issue with implementing the system were two-fold. The main issue was the sensitivity of the program, which was compounded by the time spent in terms of realising issues with the implementation. There were no obvious clues to analysing why the system may not perform as expected due to the scale of the system, and the issues were not realised until some time was spent during simulations. 

        Although the system was built where the games in the tournaments were ``embarrassingly parallel", the machine used for training did not scale as efficiently as expected. Most of the computation was spent on calculating floating-point matrix operations (Which represent the neural network evaluations.) Later research has shown that the use of hyper-threading does not necessarily show to scale the performance, even though more threads exist. \cite{leng_empirical_2002} This was due to the shared floating-point arithmetic registers in the CPU, where two threads utilise the registers of one physical core. The use of GPU acceleration was not considered due to the lack of availability, and the added complexity involved for its implementation. 
    
% TODO (WAIT FOR RESULTS)
\section{Results}
    %   Results:
    %   - Does it work?
    %   - Perhaps some measurable outcomes where appropriate.
    % this section presents the results of the solutions.  It should include information on experimental settings.  The results should demonstrate the claimed benefits/disadvantages of the proposed solutions.
    

    % This section should be between 2 to 3 pages in length.
    % need to talk about how we mwasure the success of the system
    For the experiment, three different neuroevolutionary systems were created, each running at different ply-depths. All systems run for 200 generations. For Figures containing three charts, each chart represents the results of the agents playing at 1-ply, 3-ply, and 6-ply, respectively. Each simulation utilised a population of 15 agents.

    \subsection{Learning Rate}
    % chart of performance over time
    
    The learning rate was derived by comparing the i$^{th}$ generation's champions with its predecessor champions from the previous five generations [$i-1, \cdots, i-5$]. For each previous champion, six games were played, with the outcome of the current champion's performance scored and tallied up. An average of the outcomes was deduced, representing the learning rate. The learning rates of the first 2 champions were not measured due to insufficient comparisons. This measurement was performed for every generation until the end of training. These measurements do not influence the actual learning process of the system, but were used as one of the indicators of whether the system was learning in ability over time, i.e. whether the new champion was better than its predecessors. These results were plotted cumulatively to show the overall learning rate.
    
    \begin{figure}[!ht]
        \centering
        \includegraphics[width=140mm]{images/results/combined_cummulative.pdf}
        \caption{A chart showing the cumulative learning rate across the generations for the different plys.\label{cum_growth}}
    \end{figure}
    
    A cumulative growth across all ply-depths can be seen such that returns are in the net positive as more generations are added. Furthermore, all ply-depths show a positive trend line. This suggests that the system was more likely to show an increase in ability at every generation as opposed to a reduction. Interestingly, the 3-ply shows a slower learning rate than the other plys. However, even with safe mutations, it is shown that the performance over time was volatile for all ply-depths. This could potentially be prevented by creating a new set of agents and restarting the generation's tournament when a loss was seen. Further measurements were taken to understand the reasoning of the potential troughs in performance. 
    
    \subsection{Performance}
    To measure the overall system performance, the champion of the final generation was compared against a subset of their much earlier predecessors to measure the retention of move heuristics over time. For each predecessor, the trained system played 128 games (half as black and the other for white), and the number of wins, losses and draws were counted. For debugging purposes, all agents played against a randomly playing program to determine whether the system correctly evaluated moves. A trend line was produced (with a cubic best fit equation) for the non-random games.
    % compare the bots against a minimax agent
    % chart of previous champions
    \begin{figure}[!ht]
        \centering
        \includegraphics[width=53mm]{images/results/1ply/gm_net_stats.pdf}
        \includegraphics[width=53mm]{images/results/3ply/gm_net_stats.pdf}
        \includegraphics[width=53mm]{images/results/6ply/gm_net_stats.pdf}
        \caption{Charts showing the performance of the trained systems against their ancestral counterparts.\label{net_stats}}
    \end{figure}
    
    For all systems, it appears that the winning rate increased over time, with the draw rate and loss decreased over time. For plys 3 and 6, it performed significantly better than its generation 0 counterpart (when considering the percentage of losses). This was not the case for the 1-ply - where it lost more frequently than its wins and draws combined - against its generation 0 counterpart. This would be caused by the lack of oversight heuristics retained as the system does not look ahead further enough to decide what heuristics would be kept by the neural network model.
    % For the ply of 1, we can see an immediate net trend, with the end system beating earlier generations except for the first generation. 
    % % bollocks
    % This makes sense, as heuristic retention would be lost as the search depth is so low. This does however suggest that the system is worse off than playing with an initialised system (without training). For the higher plys (3,6) however, the agents managed to consistently maintain a higher chance of winning/drawing against their earlier counterparts. It is difficult to see a trend line that suggests the growing performance of the systems over time. 
    % A decreasing trend line for draws 
    % Need to talk about losses for the 6-ply

    \subsection{Champion Distribution}
        Measurements were taken to show the genome type distribution of the champions across the generations, to determine whether the champions were more likely to be chosen based on the influence of mutations and crossovers. Each of the champions were looked at, and their genomic properties were collated to show a distribution of the chances of a particular agent being the champion. Champions that were also champions from prior generations were classed as persistent. Well performing agents from previous generations (but not champions) were classed as elite. Typically, elite agents would encompass the persistent agents but for the sake of measurements they were taken separately. This was also the case for mutations; although every offspring was mutated, some offsprings were created using genomic crossovers (which were also classed separately), described in section \ref{crossover_strategy}.  

        \begin{figure}[!ht]
            \centering
            \includegraphics[width=53mm]{images/results/1ply/champ_gen_dist.pdf}
            \includegraphics[width=53mm]{images/results/3ply/champ_gen_dist.pdf}
            \includegraphics[width=53mm]{images/results/6ply/champ_gen_dist.pdf}
            \caption{Distributions of the different genomic identities across the tournament champions. \label{champ_gen_dist}}
        \end{figure}

        Interestingly, the influence of crossovers reduced when the ply increased, and the influence of mutations increased across the generations. Overall however, crossover holds the highest probability across the plys which would suggest that the use of crossover methods (the distinctive feature of genetic algorithms opposed to other evolutionary methods) combined with mutation, would increase the chances of producing champions. The influence of the learning components of genetic algorithms (crossover and mutation) increased in relation to the increasing ply. (representing 59.5\%, 59.7\% and 65.1\% of the distributions, respectively.)

    \subsection{Quality of Inheritance Methods}
        Building on from the chart provided by Figure \ref{champ_gen_dist}, the scores retrieved by the different genomic identities were compared against their earlier counterparts. This determined whether the use of genetic algorithms can overall improve the quality of neural networks. To measure this, a generation's champion was correlated with their score (the same measurement used to evaluate the learning rate in Figure \ref{cum_growth})produced when measured against their predecessors. The results are shown in Figure \ref{champ_score_distribution}. Red circles indicate individual values for their classes.

        \begin{figure}[!ht]
            \centering
            \includegraphics[width=53mm]{images/results/1ply/champ_score_distribution.pdf}
            \includegraphics[width=53mm]{images/results/3ply/champ_score_distribution.pdf}
            \includegraphics[width=53mm]{images/results/6ply/champ_score_distribution.pdf}
            \caption{Box-Plots of the different genomic identities and their distribution of scores. The yellow line in the box represents the mean score. \label{champ_score_distribution}}
        \end{figure}

        Unsurprisingly, crossovers had a larger range of possible values compared to its mutation counterparts. This was most likely due to the weights being placed at different positions on the neural network, disrupting any heuristics it would have otherwise devised. However, the mean crossover score suggested that it negatively impacted the learning rate. Furthermore, the range and mean scores for the elite genomes (which in this specific description includes persistent agents) suggests that they were more likely to increase the learning rate.

    \subsection{Other Observations}
        \subsubsection{Number of Moves}
            The number of moves was measured to verify that the system was able to produce quality moves across the training phase. Figure \ref{move_chart} shows the distribution of moves for the games played over the generations.

            \begin{figure}[!ht]
                \centering
                \includegraphics[width=53mm]{images/results/1ply/moves.pdf}
                \includegraphics[width=53mm]{images/results/3ply/moves.pdf}
                \includegraphics[width=53mm]{images/results/6ply/moves.pdf}
                \caption{2D Histograms representing the number of moves played in the games played in the generations. \label{move_chart}}
            \end{figure}

            % The number of moves is consistent throughout as the players are evaluating by projecting their own abilities to the opponment, leading to a fairly even distribution of moves across the generations. 
            From the charts, the median number of moves in a game for 1-ply was around 50. 
            For the higher plys, the median was not as tightly grouped together. When combined with the mean number of moves, suggested that when both players chose stronger quality moves, their games lasted longer. The mean number of games for both 3 and 6 plys were similar. However, the similarity of the two charts was most likely caused by the hard limit imposed by the game rules (see \textit{\ref{tournament_selection} Tournament Selection}), where draws were forced when a piece was not taken after 100 moves. 
            As the system was shown to scale in quality of moves across the generations, this verified that the system was choosing moves at a consistent quality throughout. This consequently confirmed that the tree-search algorithm was implemented correctly, reducing the unknowns considered during evaluation.

        \subsubsection{CPU Times}

        \begin{figure}[!ht]
            \centering
    
                \begin{tabular}{ccccc}
                       & 1-Ply        & 3-Ply          & 6-Ply          & \\ \cline{2-4}
                \multicolumn{1}{c|}{Mean} & \multicolumn{1}{r|}{0:04:24} & \multicolumn{1}{r|}{0:17:32}  & \multicolumn{1}{r|}{0:38:45}   & \\ \cline{2-4}
                \multicolumn{1}{c|}{Net} & \multicolumn{1}{r|}{7:21:25} & \multicolumn{1}{r|}{1 day, 5:14:41} & \multicolumn{1}{r|}{5 days, 9:12:25} & \\ \cline{2-4}
                       &          &           &            & 
                \end{tabular}
    
            \caption{Mean and Net running times of system training for the various depths. \label{cpu_table}}
            \end{figure}


            To measure the time complexity of the program, time measurements were taken for every game played, and the time length for a generation to finish the tournament. As expected, 
            % rewrite it doesn't even make sense
            the system was shown to scale in a relatively linear fashion according to Figure \ref{cpu_table}. This was most likely caused by growing the search limit for MCTS in a linear fashion based on the ply.

            \begin{figure}[!ht]
                \centering
                \includegraphics[width=53mm]{images/results/1ply/simulation_timings.pdf}
                \includegraphics[width=53mm]{images/results/3ply/simulation_timings.pdf}
                \includegraphics[width=53mm]{images/results/6ply/simulation_timings.pdf}
                \caption{2D Histograms of the average times spent on the games during the simulations. \label{chart_cpu_times}}
            \end{figure}

            Figure \ref{chart_cpu_times} suggests that the CPU time was unstable during training, but further analysis suggests that it did not correlate with the performance of the system. This was most likely related to the use of a time-shared machine (Durham's MIRA) to run the simulations, which may throttle the amount of resources for the system for other users with their own programs. %so what?

% TODO (WAIT FOR RESULTS)
\section{Evaluation}
    % Structure and content very project-dependent.
    % Follow the Design Report. Use the feedback.
    % Rule of thumb:
 
    %   Evaluation:
    %   - How well does it work?
    %   - Does it do what you wanted it to do?
    %   - How well did you do?
    %   - Did the plans pan out?

    The original research question is now reflected upon: ``is it possible to create a performing draughts playing agent by the use of Genetic Algorithms and Neural Networks?".  The question is answered through the examination of the strengths and limitations of the system.

    \subsection{Strengths}

    Figures \ref{cum_growth}, \ref{net_stats} and \ref{champ_gen_dist} suggest that the system has improved in ability over time. This is pivotal to the research question, as no human intervention (i.e data suggested by humans) was used to help make the system learn. It also reinforces the idea that derivative based learning algorithms are not necessarily the only ways to train a neural network. This is possible for the case of playing Checkers, as an infinite number of games can be simulated. The use of crossovers suggested that it could potentially improve the quality of the system.
    
    This suggests that neuroevolution is a viable method for problems with vast search spaces, or problems where data is hard to access. Although this is the case for Deep learning and evolutionary computation in general, the benefit of using genetic algorithms is that the system relies on a relatively simple learning heuristic that is open to domain specific adaptation. Gradient based learning requires the function  translatable to some form of a differential sum of partial derivatives, which may not be feasible in some circumstances.

    \subsection{Limitations}
    Figure \ref{cpu_table} and \ref{chart_cpu_times} indicated a far slower learning rate than traditional supervised NN of equivalent quality. For instance, research has shown to create a notably strong Chess AI within the span of 72 hours by training a neural network against pre-classified tactics problems and predetermined grandmaster tournament games. \cite{lai_giraffe:_2015}. A comparison between the two is fundamentally difficult, however, as the system was not tested against an AI that is universally recognised to be strong.

    The crossover mechanism is a hit and miss, and is shown in figure \ref{champ_score_distribution}, on average, to show a negligible improvement in the learning rate. This was also seen in another research into the potentially negative influence of the textbook crossover mechanism \cite{emmanouilidis_comparison_2000}, where they also found it to negatively impact learning overall. The results have shown however, crossovers show a wide range of scores, including largely positive indicators (Also shown in Figure \ref{champ_score_distribution}). If provided a well devised crossover mechanism, can dramatically improve the quality of the learning over time.

    The quality of the neuroevolutionary algorithm boils down to the quality of the tournament method. Round-Robin is not necessarily the most efficient (from a number of games played perspective, ignoring  parallelism) and does not eliminate cases where there is an ambiguous decision between the champions (which could be due to several players having an equal number of points). This could also relate to the fact that the retention of heuristics necessary to win against much older predecessors were lost. This may be a fault of the tournament mechanism, where the population were made to optimise against each other, as opposed to having a common premise such as optimising the previous champion. Creating a high-quality tournament is difficult and domain specific, or rather, there is no universal fitness function for all problems.

    \subsection{Approach}
        The mechanism for devising the learning rate was more efficent than comparing the present champion against a hollistic comparison of the rest of the system. This measurement of learning rate was somewhat arbitary, as the learning rate is deduced against the population and not necessarily the system as a whole, and may best be compared against systems that are classified to play at a particular level. This would have helped in determining how strong the system was against popular AI checkers programs.

        As most of the experiments are constrained by computational power and time, it may be best to re-evaluate this concept until either computational resources are more available, such as the use of GPU acceleration. The system should also be left to run for longer generations as presently the results do not show a notably strong correlation. The lack of computational resources also influenced the number of experimental testing that can be measured, such as the population size, the number of generations and the learning rate. 
        
        Another consideration for the project is to experiment with different crossover algorithms that utilise the temporal difference between inputs and outputs. This could assist in the creation of safe crossovers which could also potentially accelerate the learning rate, by retaining the best aspects of two agents.
        
        There is an incredibly vast scope of future work that would extend the project. One interesting avenue is to consider using a safer method to produce crossover mechanism, exercising other contemporary methods in machine learning. It may also be the case that more time could be spent exploring problem solving mechanisms  outside of computer science. In this particular example, the algorithms used are inspired by various biological and neuroscientific understandings. More research into this field may assist in exploring creative methods of solving issues concerning the genetic crossover mechanisms and mutation rates.
    

% TODO (WAIT FOR RESULTS)

\section{Conclusion}
    % This section summarises the main points of this paper.  Do not replicate the abstract as the conclusion.  A conclusion might elaborate on the importance of the work or suggest applications and extensions.  This section should be no more than 1 page in length.

    The project was somewhat successful in terms of fulfilling its objectives. A system was implemented using evolutionary methods, and it successfully plays checkers in a manner in which it learnt from over time. To the extent of its growth remains suspect however. 
    % Efforts to measure the learning rate is one of the non-trivial challenges that makes neuroevolution a particular field of interest, as it is inherent that it can tackle problems in a somewhat novel manner.
    The main findings of this project is as follows:

    \begin{enumerate}
    \item It is evidently possible to create a checkers playing agent that learns to play itself, using genetic algorithms and neural networks.
    % - some of the work is 
    \item  Neuroevolution is inefficient compared to their gradient based counterparts. This however was understandable due to the heavy dependency of entropy disguised as learning.
    \item  The tournament can be anything, but it is important to derive a high quality tournament mechanism.
    \item How the crossover was implemented can be a deciding factor in how the agents learn to play over time. The crossover method used has shown a negligible impact on average, but has produced a wide range of possible results, some of which could greatly increase the quality of the learning rate.
    \end{enumerate}

    The major implication is that it is a possible contender for tasks that require unsupervised learning, and that data needed to produce a strong classifier is hard to access. Provided a high quality tournament method can be designed, neuroevolution could be adapted to handle any simulation based problem, where the objective can be adapted to fit into some optimisable classifier.

\footnotesize{\bibliography{zotero}}


\end{document}
