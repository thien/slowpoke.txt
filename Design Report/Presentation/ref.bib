
@article{chellapilla_evolving_1999,
	title = {Evolving {Neural} {Networks} to {Play} {Checkers} without {Expert} {Knowledge}},
	volume = {10},
	issn = {10459227},
	url = {http://ieeexplore.ieee.org/abstract/document/809083},
	doi = {10.1109/72.809083},
	abstract = {— An experiment was conducted where neural net-works compete for survival in an evolving population based on their ability to play checkers. More specifically, multilayer feedforward neural networks were used to evaluate alternative board positions and games were played using a minimax search strategy. At each generation, the extant neural networks were paired in competitions and selection was used to eliminate those that performed poorly relative to other networks. Offspring neural networks were created from the survivors using random variation of all weights and bias terms. After a series of 250 generations, the best-evolved neural network was played against human opponents in a series of 90 games on an internet website. The neural network was able to defeat two expert-level players and played to a draw against a master. The final rating of the neural network placed it in the " Class A " category using a standard rating system. Of particular importance in the design of the experiment was the fact that no features beyond the piece differential were given to the neural networks as a priori knowledge. The process of evolution was able to extract all of the additional information required to play at this level of competency. It accomplished this based almost solely on the feedback offered in the final aggregated outcome of each game played (i.e., win, lose, or draw). This procedure stands in marked contrast to the typical artifice of explicitly injecting expert knowledge into a game-playing program. Index Terms—Alpha–beta search, checkers, evolutionary com-putation, feedforward neural networks, game playing.},
	number = {6},
	journal = {IEEE Transactions on Neural Networks},
	author = {Chellapilla, Kumar and Fogel, David},
	year = {1999},
	pmid = {18252639},
	pages = {1382--1391},
	file = {Attachment:/home/tn/Zotero/storage/UIECZQLX/Chellapilla, Fogel - 1999 - Evolving Neural Networks to Play Checkers without Expert Knowledge.pdf:application/pdf}
}

@article{schaeffer_solving_1996,
	title = {Solving the {Game} of {Checkers}},
	volume = {29},
	url = {http://books.google.com/books?hl=en&lr=&id=cYB-ra2T8i4C&oi=fnd&pg=PA119&dq=Solving+the+Game+of+Checkers&ots=H0lLXM9_gp&sig=0DO68kXG6UsGhvhLfW1a5vqz3n0},
	abstract = {In 1962, a checkers-playing program written by Arthur Samuel defeated a self-proclaimed master player, creating a sensation at the time for the fledgling field of computer science called artificial intelligence. The historical record refers to this event as having solved the game of checkers. This paper discusses achieving three different levels of solving the game: publicly (as evidenced by Samuel's results), practically (by the checkers program Chinook , the best player in the world) and provably (by consid-ering the 5 × 10 20 positions in the search space). The latter definition may be attainable in the near future.},
	journal = {Games of No Chance},
	author = {Schaeffer, Jonathan and Lake, Robert},
	year = {1996},
	pages = {119--133},
	file = {Attachment:/home/tn/Zotero/storage/4JP6WH7G/Schaeffer, Lake - 1996 - Solving the Game of Checkers.pdf:application/pdf}
}

@article{samuel_studies_2000,
	title = {Some studies in machine learning using the game of checkers},
	volume = {44},
	issn = {0018-8646},
	url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5389202%5Cnpapers3://publication/doi/10.1147/rd.441.0206},
	doi = {10.1147/rd.441.0206},
	abstract = {AL Samuel Some Studies in Machine Learning Using the Game of Checkers Abstract: Two machine - learning procedures have been investigated in some detail using the game of checkers . Enough work has been done to verify the fact that a computer can be ... \${\textbackslash}backslash\$n},
	number = {1.2},
	journal = {IBM Journal of Research and Development},
	author = {Samuel, A L},
	year = {2000},
	pages = {206--226},
	file = {Attachment:/home/tn/Zotero/storage/56UN45RQ/Samuel - 2000 - Some studies in machine learning using the game of checkers.pdf:application/pdf}
}

@article{lai_giraffe:_2015,
	title = {Giraffe: {Using} {Deep} {Reinforcement} {Learning} to {Play} {Chess}},
	url = {http://arxiv.org/abs/1509.01549},
	abstract = {This report presents Giraffe, a chess engine that uses self-play to discover all its domain-specific knowledge, with minimal hand-crafted knowledge given by the programmer. Unlike previous attempts using machine learning only to perform parameter-tuning on hand-crafted evaluation functions, Giraffe's learning system also performs automatic feature extraction and pattern recognition. The trained evaluation function performs comparably to the evaluation functions of state-of-the-art chess engines - all of which containing thousands of lines of carefully hand-crafted pattern recognizers, tuned over many years by both computer chess experts and human chess masters. Giraffe is the most successful attempt thus far at using end-to-end machine learning to play chess.},
	number = {September},
	author = {Lai, Matthew},
	year = {2015},
	file = {Attachment:/home/tn/Zotero/storage/AFW46QNS/Lai - 2015 - Giraffe Using Deep Reinforcement Learning to Play Chess.pdf:application/pdf}
}

@article{such_deep_2017,
	title = {Deep {Neuroevolution}: {Genetic} {Algorithms} {Are} a {Competitive} {Alternative} for {Training} {Deep} {Neural} {Networks} for {Reinforcement} {Learning}},
	shorttitle = {Deep {Neuroevolution}},
	url = {http://arxiv.org/abs/1712.06567},
	abstract = {Deep artificial neural networks (DNNs) are typically trained via gradient-based learning algorithms, namely backpropagation. Evolution strategies (ES) can rival backprop-based algorithms such as Q-learning and policy gradients on challenging deep reinforcement learning (RL) problems. However, ES can be considered a gradient-based algorithm because it performs stochastic gradient descent via an operation similar to a finite-difference approximation of the gradient. That raises the question of whether non-gradient-based evolutionary algorithms can work at DNN scales. Here we demonstrate they can: we evolve the weights of a DNN with a simple, gradient-free, population-based genetic algorithm (GA) and it performs well on hard deep RL problems, including Atari and humanoid locomotion. The Deep GA successfully evolves networks with over four million free parameters, the largest neural networks ever evolved with a traditional evolutionary algorithm. These results (1) expand our sense of the scale at which GAs can operate, (2) suggest intriguingly that in some cases following the gradient is not the best choice for optimizing performance, and (3) make immediately available the multitude of techniques that have been developed in the neuroevolution community to improve performance on RL problems. To demonstrate the latter, we show that combining DNNs with novelty search, which was designed to encourage exploration on tasks with deceptive or sparse reward functions, can solve a high-dimensional problem on which reward-maximizing algorithms (e.g. DQN, A3C, ES, and the GA) fail. Additionally, the Deep GA parallelizes better than ES, A3C, and DQN, and enables a state-of-the-art compact encoding technique that can represent million-parameter DNNs in thousands of bytes.},
	urldate = {2018-01-17},
	journal = {arXiv:1712.06567 [cs]},
	author = {Such, Felipe Petroski and Madhavan, Vashisht and Conti, Edoardo and Lehman, Joel and Stanley, Kenneth O. and Clune, Jeff},
	month = dec,
	year = {2017},
	note = {arXiv: 1712.06567},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing}
}

@article{winands_monte_2010,
	title = {Monte {Carlo} {Tree} {Search} in {Lines} of {Action}},
	volume = {2},
	issn = {1943-068X},
	doi = {10.1109/TCIAIG.2010.2061050},
	abstract = {The success of Monte Carlo tree search (MCTS) in many games, where αβ-based search has failed, naturally raises the question whether Monte Carlo simulations will eventually also outperform traditional game-tree search in game domains where αβ -based search is now successful. The forte of αβ-based search are highly tactical deterministic game domains with a small to moderate branching factor, where efficient yet knowledge-rich evaluation functions can be applied effectively. In this paper, we describe an MCTS-based program for playing the game Lines of Action (LOA), which is a highly tactical slow-progression game exhibiting many of the properties difficult for MCTS. The program uses an improved MCTS variant that allows it to both prove the game-theoretical value of nodes in a search tree and to focus its simulations better using domain knowledge. This results in simulations superior in both handling tactics and ensuring game progression. Using the improved MCTS variant, our program is able to outperform even the world's strongest αβ-based LOA program. This is an important milestone for MCTS because the traditional game-tree search approach has been considered to be the better suited for playing LOA.},
	number = {4},
	journal = {IEEE Transactions on Computational Intelligence and AI in Games},
	author = {Winands, M. H. M. and Bjornsson, Y. and Saito, J. T.},
	month = dec,
	year = {2010},
	keywords = {game theory, Monte Carlo methods, Monte Carlo tree search (MCTS), tree searching, computer games, Filling, game-theoretical value, game-tree search, Game-tree solver, Imaging phantoms, lines of action, Lines of Action (LOA), Monte Carlo simulations, Monte Carlo tree search, tactical slow-progression game, αβ-based search},
	pages = {239--250},
	file = {IEEE Xplore Abstract Record:/home/tn/Zotero/storage/XDJICKYR/5523941.html:text/html}
}