
@article{chellapilla_evolving_1999,
	title = {Evolving {Neural} {Networks} to {Play} {Checkers} without {Expert} {Knowledge}},
	volume = {10},
	issn = {10459227},
	url = {http://ieeexplore.ieee.org/abstract/document/809083},
	doi = {10.1109/72.809083},
	abstract = {— An experiment was conducted where neural net-works compete for survival in an evolving population based on their ability to play checkers. More specifically, multilayer feedforward neural networks were used to evaluate alternative board positions and games were played using a minimax search strategy. At each generation, the extant neural networks were paired in competitions and selection was used to eliminate those that performed poorly relative to other networks. Offspring neural networks were created from the survivors using random variation of all weights and bias terms. After a series of 250 generations, the best-evolved neural network was played against human opponents in a series of 90 games on an internet website. The neural network was able to defeat two expert-level players and played to a draw against a master. The final rating of the neural network placed it in the " Class A " category using a standard rating system. Of particular importance in the design of the experiment was the fact that no features beyond the piece differential were given to the neural networks as a priori knowledge. The process of evolution was able to extract all of the additional information required to play at this level of competency. It accomplished this based almost solely on the feedback offered in the final aggregated outcome of each game played (i.e., win, lose, or draw). This procedure stands in marked contrast to the typical artifice of explicitly injecting expert knowledge into a game-playing program. Index Terms—Alpha–beta search, checkers, evolutionary com-putation, feedforward neural networks, game playing.},
	number = {6},
	journal = {IEEE Transactions on Neural Networks},
	author = {Chellapilla, Kumar and Fogel, David},
	year = {1999},
	pmid = {18252639},
	pages = {1382--1391},
	file = {Attachment:/home/tn/Zotero/storage/UIECZQLX/Chellapilla, Fogel - 1999 - Evolving Neural Networks to Play Checkers without Expert Knowledge.pdf:application/pdf}
}

@phdthesis{allsop_artficial_2013,
	title = {Artficial {Intelligence} {Techniques} {Applied} {To} {Draughts}},
	school = {Durham University},
	author = {Allsop, Daniel (Durham University)},
	year = {2013},
	file = {Attachment:/home/tn/Zotero/storage/VN5S9LCQ/Allsop - 2013 - Artficial Intelligence Techniques Applied To Draughts Artificial Intelligence Techniques Applied To Draughts.pdf:application/pdf}
}

@article{thesis_combining_1994,
	title = {Combining {Genetic} {Algorithms} and {Neural} {Networks} : {The} {Encoding} {Problem}},
	abstract = {Neural networks and genetic algorithms demonstrate powerful problem solving ability. They are based on quite simple principles, but take advantage of their mathematical nature: non-linear iteration. Neural networks with backpropagation learning showed results by searching for various kinds of functions. However, the choice of the basic parameter (network topology, learning rate, initial weights) often already determines the success of the training process. The selection of these parame- ter follow in practical use rules of thumb, but their value is at most arguable. Genetic algorithms are global search methods, that are based on princi- ples like selection, crossover and mutation. This thesis examines how genetic algorithms can be used to optimize the network topology etc. of neural net- works. It investigates, how various encoding strategies influence the GA/NN synergy. They are evaluated according to their performance on academic and practical problems of different complexity.},
	number = {December},
	journal = {Thesis ANN p 67},
	author = {Thesis, a},
	year = {1994},
	pages = {1--67},
	file = {Attachment:/home/tn/Zotero/storage/NUFQSZFS/Thesis - 1994 - Combining Genetic Algorithms and Neural Networks The Encoding Problem.pdf:application/pdf}
}

@article{perez_apply_nodate,
	title = {Apply genetic algorithm to the learning phase of a neural network},
	abstract = {Natural networks have been used during several years to solve classifica-tion problems. The performance of a neural network depends directly on the design of the hidden layers, and in the calculation of the weights that connect the different nodes. On this project, the structure of the hidden layer is not modified, as the interest lies only on the calculation of the weights of the system. In order to obtain a feasible result, the weights of the neural network are calculated due a function cost. A genetic algo-rithm approach is presented and compared with the gradient descent in failure rate and time to obtain a solution.},
	author = {Perez, Sergi},
	file = {Attachment:/home/tn/Zotero/storage/YLCQSUUZ/Perez - Unknown - Apply genetic algorithm to the learning phase of a neural network.pdf:application/pdf}
}

@article{samuel_studies_2000,
	title = {Some studies in machine learning using the game of checkers},
	volume = {44},
	issn = {0018-8646},
	url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5389202%5Cnpapers3://publication/doi/10.1147/rd.441.0206},
	doi = {10.1147/rd.441.0206},
	abstract = {AL Samuel Some Studies in Machine Learning Using the Game of Checkers Abstract: Two machine - learning procedures have been investigated in some detail using the game of checkers . Enough work has been done to verify the fact that a computer can be ... \${\textbackslash}backslash\$n},
	number = {1.2},
	journal = {IBM Journal of Research and Development},
	author = {Samuel, A L},
	year = {2000},
	pages = {206--226},
	file = {Attachment:/home/tn/Zotero/storage/56UN45RQ/Samuel - 2000 - Some studies in machine learning using the game of checkers.pdf:application/pdf}
}

@article{lai_giraffe:_2015,
	title = {Giraffe: {Using} {Deep} {Reinforcement} {Learning} to {Play} {Chess}},
	url = {http://arxiv.org/abs/1509.01549},
	abstract = {This report presents Giraffe, a chess engine that uses self-play to discover all its domain-specific knowledge, with minimal hand-crafted knowledge given by the programmer. Unlike previous attempts using machine learning only to perform parameter-tuning on hand-crafted evaluation functions, Giraffe's learning system also performs automatic feature extraction and pattern recognition. The trained evaluation function performs comparably to the evaluation functions of state-of-the-art chess engines - all of which containing thousands of lines of carefully hand-crafted pattern recognizers, tuned over many years by both computer chess experts and human chess masters. Giraffe is the most successful attempt thus far at using end-to-end machine learning to play chess.},
	number = {September},
	author = {Lai, Matthew},
	year = {2015},
	file = {Attachment:/home/tn/Zotero/storage/AFW46QNS/Lai - 2015 - Giraffe Using Deep Reinforcement Learning to Play Chess.pdf:application/pdf}
}

@book{mullins_checkers_2007,
	title = {Checkers 'solved' after years of number crunching},
	url = {https://www.newscientist.com/article/dn12296-checkers-solved-after-years-of-number-crunching/ http://www.newscientisttech.com/article/dn12296-checkers-solved-after-years-of-number-crunching.html},
	author = {Mullins, Justin},
	year = {2007}
}

@article{lorentz_using_2016,
	title = {Using evaluation functions in {Monte}-{Carlo} {Tree} {Search}},
	volume = {644},
	issn = {03043975},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0304397516302717},
	doi = {10.1016/j.tcs.2016.06.026},
	language = {en},
	urldate = {2018-01-06},
	journal = {Theoretical Computer Science},
	author = {Lorentz, Richard},
	month = sep,
	year = {2016},
	pages = {106--113},
	file = {1-s2.0-S0304397516302717-main.pdf:/home/tn/Zotero/storage/LF6KY5WA/1-s2.0-S0304397516302717-main.pdf:application/pdf}
}

@article{browne_survey_2012,
	title = {A {Survey} of {Monte} {Carlo} {Tree} {Search} {Methods}},
	volume = {4},
	issn = {1943-068X},
	doi = {10.1109/TCIAIG.2012.2186810},
	abstract = {Monte Carlo tree search (MCTS) is a recently proposed search method that combines the precision of tree search with the generality of random sampling. It has received considerable interest due to its spectacular success in the difficult problem of computer Go, but has also proved beneficial in a range of other domains. This paper is a survey of the literature to date, intended to provide a snapshot of the state of the art after the first five years of MCTS research. We outline the core algorithm's derivation, impart some structure on the many variations and enhancements that have been proposed, and summarize the results from the key game and nongame domains to which MCTS methods have been applied. A number of open research questions indicate that the field is ripe for future work.},
	number = {1},
	journal = {IEEE Transactions on Computational Intelligence and AI in Games},
	author = {Browne, C. B. and Powley, E. and Whitehouse, D. and Lucas, S. M. and Cowling, P. I. and Rohlfshagen, P. and Tavener, S. and Perez, D. and Samothrakis, S. and Colton, S.},
	month = mar,
	year = {2012},
	keywords = {Artificial intelligence, Artificial intelligence (AI), bandit-based methods, computer Go, Computers, Decision theory, game search, game theory, Game theory, Games, key game, Markov processes, MCTS research, Monte Carlo methods, Monte Carlo tree search (MCTS), Monte carlo tree search methods, nongame domains, random sampling generality, tree searching, upper confidence bounds (UCB), upper confidence bounds for trees (UCT)},
	pages = {1--43},
	file = {IEEE Xplore Abstract Record:/home/tn/Zotero/storage/X2QAE6CV/6145622.html:text/html}
}

@inproceedings{montana_training_1989,
	title = {Training {Feedforward} {Neural} {Networks} {Using} {Genetic} {Algorithms}.},
	volume = {89},
	booktitle = {{IJCAI}},
	author = {Montana, David J. and Davis, Lawrence},
	year = {1989},
	pages = {762--767},
	file = {122.pdf:/home/tn/Zotero/storage/EZIWHW97/122.pdf:application/pdf}
}

@article{such_deep_2017,
	title = {Deep {Neuroevolution}: {Genetic} {Algorithms} {Are} a {Competitive} {Alternative} for {Training} {Deep} {Neural} {Networks} for {Reinforcement} {Learning}},
	shorttitle = {Deep {Neuroevolution}},
	url = {http://arxiv.org/abs/1712.06567},
	abstract = {Deep artificial neural networks (DNNs) are typically trained via gradient-based learning algorithms, namely backpropagation. Evolution strategies (ES) can rival backprop-based algorithms such as Q-learning and policy gradients on challenging deep reinforcement learning (RL) problems. However, ES can be considered a gradient-based algorithm because it performs stochastic gradient descent via an operation similar to a finite-difference approximation of the gradient. That raises the question of whether non-gradient-based evolutionary algorithms can work at DNN scales. Here we demonstrate they can: we evolve the weights of a DNN with a simple, gradient-free, population-based genetic algorithm (GA) and it performs well on hard deep RL problems, including Atari and humanoid locomotion. The Deep GA successfully evolves networks with over four million free parameters, the largest neural networks ever evolved with a traditional evolutionary algorithm. These results (1) expand our sense of the scale at which GAs can operate, (2) suggest intriguingly that in some cases following the gradient is not the best choice for optimizing performance, and (3) make immediately available the multitude of techniques that have been developed in the neuroevolution community to improve performance on RL problems. To demonstrate the latter, we show that combining DNNs with novelty search, which was designed to encourage exploration on tasks with deceptive or sparse reward functions, can solve a high-dimensional problem on which reward-maximizing algorithms (e.g. DQN, A3C, ES, and the GA) fail. Additionally, the Deep GA parallelizes better than ES, A3C, and DQN, and enables a state-of-the-art compact encoding technique that can represent million-parameter DNNs in thousands of bytes.},
	urldate = {2018-01-17},
	journal = {arXiv:1712.06567 [cs]},
	author = {Such, Felipe Petroski and Madhavan, Vashisht and Conti, Edoardo and Lehman, Joel and Stanley, Kenneth O. and Clune, Jeff},
	month = dec,
	year = {2017},
	note = {arXiv: 1712.06567},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing}
}

@inproceedings{nair_rectified_2010,
	title = {Rectified linear units improve restricted boltzmann machines},
	booktitle = {Proceedings of the 27th international conference on machine learning ({ICML}-10)},
	author = {Nair, Vinod and Hinton, Geoffrey E.},
	year = {2010},
	pages = {807--814},
	file = {reluICML.pdf:/home/tn/Zotero/storage/KQTHHAUM/reluICML.pdf:application/pdf}
}

@article{baxter_tdleaflambda_1999,
	title = {{TDLeaf}(lambda) : {Combining} {Temporal} {Difference} {Learning} with {Game}-{Tree} {Search}},
	shorttitle = {{TDLeaf}(lambda)},
	url = {http://arxiv.org/abs/cs/9901001},
	abstract = {In this paper we present TDLeaf(lambda), a variation on the TD(lambda) algorithm that enables it to be used in conjunction with minimax search. We present some experiments in both chess and backgammon which demonstrate its utility and provide comparisons with TD(lambda) and another less radical variant, TD-directed(lambda). In particular, our chess program, ``KnightCap,'' used TDLeaf(lambda) to learn its evaluation function while playing on the Free Internet Chess Server (FICS, fics.onenet.net). It improved from a 1650 rating to a 2100 rating in just 308 games. We discuss some of the reasons for this success and the relationship between our results and Tesauro's results in backgammon.},
	urldate = {2018-01-23},
	journal = {arXiv:cs/9901001},
	author = {Baxter, Jonathan and Tridgell, Andrew and Weaver, Lex},
	month = jan,
	year = {1999},
	note = {arXiv: cs/9901001},
	keywords = {Computer Science - Learning, Computer Science - Artificial Intelligence, I.2.6},
	file = {arXiv\:cs/9901001 PDF:/home/tn/Zotero/storage/HNX8FM8K/Baxter et al. - 1999 - TDLeaf(lambda) Combining Temporal Difference Lear.pdf:application/pdf;arXiv.org Snapshot:/home/tn/Zotero/storage/H6EKDMHX/9901001.html:text/html}
}

@article{winands_monte_2010,
	title = {Monte {Carlo} {Tree} {Search} in {Lines} of {Action}},
	volume = {2},
	issn = {1943-068X},
	doi = {10.1109/TCIAIG.2010.2061050},
	abstract = {The success of Monte Carlo tree search (MCTS) in many games, where αβ-based search has failed, naturally raises the question whether Monte Carlo simulations will eventually also outperform traditional game-tree search in game domains where αβ -based search is now successful. The forte of αβ-based search are highly tactical deterministic game domains with a small to moderate branching factor, where efficient yet knowledge-rich evaluation functions can be applied effectively. In this paper, we describe an MCTS-based program for playing the game Lines of Action (LOA), which is a highly tactical slow-progression game exhibiting many of the properties difficult for MCTS. The program uses an improved MCTS variant that allows it to both prove the game-theoretical value of nodes in a search tree and to focus its simulations better using domain knowledge. This results in simulations superior in both handling tactics and ensuring game progression. Using the improved MCTS variant, our program is able to outperform even the world's strongest αβ-based LOA program. This is an important milestone for MCTS because the traditional game-tree search approach has been considered to be the better suited for playing LOA.},
	number = {4},
	journal = {IEEE Transactions on Computational Intelligence and AI in Games},
	author = {Winands, M. H. M. and Bjornsson, Y. and Saito, J. T.},
	month = dec,
	year = {2010},
	keywords = {computer games, Filling, game theory, game-theoretical value, game-tree search, Game-tree solver, Imaging phantoms, lines of action, Lines of Action (LOA), Monte Carlo methods, Monte Carlo simulations, Monte Carlo tree search, Monte Carlo tree search (MCTS), tactical slow-progression game, tree searching, αβ-based search},
	pages = {239--250},
	file = {IEEE Xplore Abstract Record:/home/tn/Zotero/storage/XDJICKYR/5523941.html:text/html}
}

@book{swiechowski_recent_2015,
	title = {Recent {Advances} in {General} {Game} {Playing}},
	volume = {2015},
	abstract = {The goal of General Game Playing (GGP) has been to develop computer programs that can perform well across various game types. It is natural for human game players to transfer knowledge from games they already know how to play to other similar games. GGP research attempts to design systems that work well across different game types, including unknown new games. In this review, we present a survey of recent advances (2011 to 2014) in GGP for both traditional games and video games. It is notable that research on GGP has been expanding into modern video games. Monte-Carlo Tree Search and its enhancements have been the most influential techniques in GGP for both research domains. Additionally, international competitions have become important events that promote and increase GGP research. Recently, a video GGP competition was launched. In this survey, we review recent progress in the most challenging research areas of Artificial Intelligence (AI) related to universal game playing.},
	author = {Swiechowski, Maciej and Park, HyunSoo and Mandziuk, Jacek and Kim, Kyung-Joong},
	month = sep,
	year = {2015},
	note = {DOI: 10.1155/2015/986262}
}